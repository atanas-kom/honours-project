me013: We're going? OK.
me006: OK.
me013: Sh- Close your door on - door on the way out? Thanks.
me006: Thanks. Oh.
me013: Yeah. Probably wanna get this other door, too. OK. So. Um. What are we talking about today?
mn007: Uh, well, first there are perhaps these uh Meeting Recorder digits
me013: Oh, yeah. That was kind of uh interesting. The - both the uh -
mn007: that we tested. So. Um.
me013: the SRI System and the oth- And for one thing that - that sure shows the difference between having a lot of uh
mn007: Of data?
me013: training data
mn007: Yeah.
me013: or not, uh, the uh - The best kind of number we have on the English uh - on near microphone only is - is uh three or four percent.
mn007: Mm-hmm.
me013: And uh it's significantly better than that, using fairly simple front-ends on - on the uh -
mn007: Mm-hmm.
me013: uh, with the SRI system. So I th- I think that the uh - But that's - that's using uh a - a pretty huge amount of data, mostly not digits, of course, but - but then again - Well, yeah. In fact , mostly not digits for the actual training the HM Ms whereas uh in this case we're just using digits for training the HMMs.
mn007: Yeah. Right.
me013: Did anybody mention about whether the - the SRI system is a - is - is doing the digits um the wor- as a word model or as uh a sub- s- sub-phone states?
mn007: I guess it's - it's uh allophone models, so, well -
me013: Yeah. Probably. Huh?
mn007: Yeah. I think so, because it's their very d- huge, their huge system.
me013: Yeah.
mn007: And. But. So. There is one difference - Well, the SRI system - the result for the SRI system that are represented here are with adaptation. So there is - It's their complete system and - including on-line uh unsupervised adaptation.
me013: That's true.
mn007: And if you don't use adaptation, the error rate is around fifty percent worse, I think, if I remember. Yeah.
me013: OK. It's tha- it's that much, huh?
mn007: Nnn. It's - Yeah. It's quite significant. Yeah.
me013: Oh. OK. Still.
mn007: Mm-hmm.
me013: But - but uh what - what I think I'd be interested to do given that, is that we - we should uh take - I guess that somebody's gonna do this, right? - is to take some of these tandem things and feed it into the SRI system, right?
mn007: Yeah.
me013: Yeah.
mn007: We can do something like that . Yeah.
me013: Yeah. Because -
mn007: But - But I guess the main point is the data because uh I am not sure. Our back-end is - is fairly simple but until now, well, the attempts to improve it or - have fail- Ah, well, I mean uh what Chuck tried to - to - to do
me013: Yeah, but he's doing it with the same data, right? I mean so to -
mn007: Yeah. So it's - Yeah.
me013: So there's - there's - there's two things being affected. I mean. One is that - that, you know, there's something simple that's wrong with the back-end. We've been playing a number of states uh I - I don't know if he got to the point of playing with the uh number of Gaussians yet but - but uh,
mn007: Mm-hmm. Mm-hmm.
me013: uh, you know. But, yeah, so far he hadn't gotten any big improvement, but that's all with the same amount of data which is pretty small.
mn007: Mm-hmm. Yeah. Mmm.
me013: And um.
mn007: So, yeah, we could retrain some of these tandem
me013: Well, you could do that, but I'm saying even with it not - with that part not retrained,
mn007: on - on huge - Ah, yeah. Just -
me013: just - just using - having the HMMs -
mn007: f- for the HMM models. Yeah. Mm-hmm.
me013: much better HM Ms. Yeah.
mn007: Mm-hmm.
me013: Um. But just train those HMMs using different features, the features coming from our Aurora stuff. So.
mn007: Yeah. Yeah. But what would be interesting to see also is what - what - perhaps it's not related, the amount of data but the um recording conditions. I don't know. Because it's probably not a problem of noise, because our features are supposed to be robust to noise.
me013: Well, yeah.
mn007: It's not a problem of channel, because there is um normalization with respect to the channel. So -
me013: I - I - I'm sorry. What - what is the problem that you're trying to explain?
mn007: The - the fact that - the result with the tandem and Aurora system are
me013: That the - Oh. So much worse?
mn007: uh so much worse. Yeah.
me013: Oh. I uh but I'm - I'm almost certain that it - it -
mn007: It -
me013: I mean, that it has to do with the um amount of training data. It - it's - it's orders of magnitude off.
mn007: Yeah but - Yeah. Yeah but we train only on digits and it's - it's a digit task, so. Well.
me013: But - but having a huge - If -
mn007: It -
me013: if you look at what commercial places do, they use a huge amount of data. This is a modest amount of data.
mn007: Mm-hmm. Alright. Yeah. Mm-hmm.
me013: So. I mean, ordinarily you would say "well, given that you have enough occurrences of the digits, you can just train with digits rather than with, you know" -
mn007: Mm-hmm.
me013: But the thing is, if you have a huge - in other words, do word models - But if you have a huge amount of data
mn007: Right. Mmm.
me013: then you're going to have many occurrences of similar uh allophones.
mn007: Yeah.
me013: And that's just a huge amount of training for it. So it's um -
mn007: Mm-hmm.
me013: I - I think it has to be that, because, as you say, this is, you know, this is near-microphone, it's really pretty clean data.
mn007: Mm-hmm.
me013: Um. Now, some of it could be the fact that uh - let's see, in the - in these multi-train things did we include noisy data in the
mn007: Yeah.
me013: training? I mean, that could be hurting us actually, for the clean case.
mn007: Yeah. Well, actually we see that the clean train for the Aurora proposals are -
me013: It is if -
mn007: are better than the multi-train, yeah.
me013: Yeah. Yeah. Cuz this is clean data, and so that's not too surprising.
mn007: Mm-hmm.
me013: But um. Uh. So.
mn007: Well, o- I guess what I meant is that well, let's say if we - if we add enough data to train on the um
me013: Uh-huh.
mn007: on the Meeting Recorder digits,
me013: Mm-hmm.
mn007: I guess we could have better results than this. And. What I meant is that perhaps we can learn something uh from this, what's - what's wrong uh what - what is different between TI-digits and these digits and -
me013: What kind of numbers are we getting on TI-digits?
mn007: It's point eight percent, so.
me013: Oh. I see.
mn007: Four- Fourier. @@
me013: So in the actual TI-digits database we're getting point eight percent,
mn007: Yeah. Yeah.
me013: and here we're getting three or four - three, let's see, three for this?
mn007: Mm-hmm.
me013: Yeah. Sure, but I mean, um point eight percent is something like double uh or triple what people have gotten who've worked very hard at doing that. And - and also, as you point out, there's adaptation in these numbers also.
mn007: Mm-hmm. Mmm.
me013: So if you, you know, put the ad- adap- take the adaptation off, then it - for the English-Near you get something like two percent. And here you had, you know, something like three point four.
mn007: Mm-hmm.
me013: And I could easily see that difference coming from this huge amount of data that it was trained on. So it's -
mn007: Mm-hmm.
me013: You know, I don't think there's anything magical here. It's, you know, we used a simple HTK system with a modest amount of data. And this is a - a, you know, modern uh system uh has - has a lot of nice points to it.
mn007: Yeah. Yeah. Mm-hmm.
me013: Um. So. I mean, the HTK is an older HTK, even. So.
mn007: Mm-hmm.
me013: Yeah it - it's not that surprising. But to me it just - it just meant a practical point that um if we want to publish results on digits that - that people pay attention to we probably should uh - Cuz we've had the problem before that you get - show some nice improvement on something that's - that's uh, uh - it seems like too large a number, and uh uh people don't necessarily take it so seriously.
mn007: Mm-hmm.
me013: Um. Yeah. Yeah. So the three point four percent for this uh is - is uh - So why is it - It's an interesting question though, still. Why is - why is it three point four percent for the d- the digits recorded in this environment as opposed to the uh point eight percent for - for - for the original TI-digits database? Um.
mn007: Yeah. th- that's - th- that's my point I - I - I don't
me013: Given - given the same - Yeah. So ignore - ignoring the - the - the SRI system for a moment, just looking at
mn007: I - Mm-hmm.
me013: the TI-di- the uh tandem system, if we're getting point eight percent, which, yes, it's high. It's, you know, it - it's not awfully high, but it's, you know - it's - it's high.
mn007: Mm-hmm.
me013: Um. Why is it uh four times as high, or more?
mn007: Yeah, I guess.
me013: Right? I mean, there's - even though it's close-miked there's still - there really is background noise.
mn007: Mm-hmm.
me013: Um. And uh I suspect when the TI-digits were recorded if somebody fumbled or said something wrong or something that they probably made them take it over.
mn007: Mm-hmm.
me013: It was not - I mean there was no attempt to have it be realistic in any - in any sense at all.
mn007: Well. Yeah. And acoustically, it's q- it's - I listened. It's quite different. TI-digit is - it's very, very clean and it's like studio recording
me013: Mm-hmm.
mn007: whereas these Meeting Recorder digits sometimes you have breath noise and Mmm.
me013: Right. Yeah. So I think they were -
mn007: It's not controlled at all, I mean.
me013: Bless you.
me006: Thanks.
me013: I - Yeah. I think it's - it's -
mn007: Mm-hmm. But-
me013: So. Yes. It's - I think it's - it's the indication it's harder.
mn007: Yeah.
me013: Uh. Yeah and again, you know, i- that's true either way. I mean so take a look at the uh - um, the SRI results. I mean, they're much much better, but still you're getting something like one point three percent
mn007: Mm-hmm.
me013: for uh things that are same data as in T - TI-digits the same - same text. Uh. And uh, I'm sure the same - same system would - would get, you know, point - point three or point four or something on the actual TI-digits. So this - I think, on both systems the these digits are showing up as harder.
mn007: Mmm.
me013: Um.
mn007: Mm-hmm.
me013: Which I find sort of interesting cause I think this is closer to - uh I mean it's still read. But I still think it's much closer to - to what - what people actually face, um when they're - they're dealing with people saying digits over the telephone. I mean. I don't think uh - I mean, I'm sure they wouldn't release the numbers, but I don't think that uh the uh - the - the companies that - that do telephone speech get anything like point four percent on their digits. I'm - I'm - I'm sure they get - Uh, I mean, for one thing people do phone up who don't have uh uh Middle America accents and
mn007: Mm-hmm.
me013: it's a we- we it's - it's - it's US. it has - has many people who sound in many different ways. So. Um. I mean. OK. That was that topic. What else we got?
mn007: Um. But -
me013: Did we end up giving up on - on, any Eurospeech submissions, or - ? I know Thilo and Dan Ellis are - are submitting something, but uh.
mn007: Yeah. I - I guess e- the only thing with these - the Meeting Recorder and, well, - So, I think, yeah - I think we basically gave up.
me013: Um.
mn007: But -
me013: Now, actually for the - for the Aur- uh we do have stuff for Aurora, right? Because - because we have ano- an extra month or something.
mn007: Yeah. Yeah. Yeah. So. Yeah, for sure we will
me013: Yeah.
mn007: do something for
me013: Well, that's fine. So th- so - so we have a couple - a couple little things on Meeting Recorder and we have -
mn007: the special session. Yeah. Mm-hmm.
me013: We don't - we don't have to flood it with papers. We're not trying to prove anything to anybody. so. That's fine. Um. Anything else?
mn007: Yeah. Well. So. Perhaps the point is that we've been working on is, yeah, we have put the um the good VAD in the system and it really makes a huge difference. Um. So, yeah. I think, yeah, this is perhaps one of the reason why our system was not - not the best, because with the new VAD, it's very - the results are similar to the France Telecom results and
me013: Hmm.
mn007: perhaps even better sometimes.
me006: Huh.
mn007: Um. So there is this point. Uh. The problem is that it's very big and we still have to think how to - where to put it and - um,
me013: Mm-hmm.
mn007: because it - it - well, this VAD uh either some delay and we - if we put it on the server side, it doesn't work, because on the server side features you already have LDA applied from the f- from the terminal side and so you accumulate the delay so the VAD should be before the LDA which means perhaps on the terminal side and then smaller and
me013: So wha- where did this good VAD come from?
mn007: So. It's um from OGI. So it's the network trained - it's the network with the huge amounts on hidden - of hidden units, and um nine input frames compared to the VAD that was in the proposal which has a very small amount of hidden units and fewer inputs.
me013: This is the one they had originally? Oh.
mn007: Yeah.
me013: Yeah, but they had to get rid of it because of the space, didn't they?
mn007: Yeah. So. Yeah. But the abso- assumption is that we will be able to make a VAD that's small and that works fine. And.
me013: Well. So that's a problem. Yeah.
mn007: So we can - Yeah but - nnn.
me013: But the other thing is uh to use a different VAD entirely. I mean, uh i- if - if there's a if - if - I - I don't know what the thinking was amongst the - the - the the ETSI folk but um
mn007: Mm-hmm.
me013: if everybody agreed sure let's use this VAD and take that out of there -
mn007: Mm-hmm. They just want, apparently - they don't want to fix the VAD because they think there is some interaction between feature extraction and - and VAD or frame dropping But they still want to - just to give some um requirement for this VAD because it's - it will not be part of - they don't want it to be part of the standard.
me013: OK.
mn007: So. So it must be at least uh somewhat fixed but not completely. So there just will be some requirements that are still not - uh not yet uh ready I think.
me013: Determined. I see.
mn007: Nnn.
me013: But I was thinking that - that uh s "Sure, there may be some interaction, but I don't think we need to be stuck on using our or OGI's VAD. We could use somebody else's if it's smaller or -
mn007: Yeah.
me013: You know, as long as it did the job.
mn007: Mm-hmm.
me013: So that's good.
mn007: Uh. So there is this thing. There is um - Yeah. Uh I designed a new - a new filter because when I designed other filters with shorter delay from the LDA filters, there was one filter with fif- sixty millisecond delay and the other with ten milliseconds and
me013: Right.
mn007: uh Hynek suggested that both could have sixty-five sixty-s- I think it's sixty-five. Yeah.
me013: Yeah.
mn007: Both should have sixty-five because - Yeah.
me013: You didn't gain anything, right?
mn007: And. So I did that and uh it's running. So, let's see what will happen. Uh but the filter is of course closer to the reference filter.
me013: Mm-hmm.
mn007: Mmm. Um. Yeah. I think -
me013: So that means logically, in principle, it should be better. So probably it'll be worse.
mn007: Yeah
me013: Or in the basic perverse nature uh of reality. Yeah. OK.
mn007: Yeah. Sure.
me026: Yeah.
me013: OK.
mn007: Yeah, and then we've started to work with this of um voiced-unvoiced
me013: Mm-hmm.
mn007: stuff. And next week I think we will perhaps try to have um a new system with uh uh MSG stream also see what - what happens. So, something that's similar to the proposal too, but with MSG stream.
me013: Mm-hmm. Mm-hmm.
mn007: Mmm.
me013: OK.
fn002: No, I w- I begin to play with Matlab and to found some parameter robust for voiced-unvoiced decision. But only to play. And we - they - we found that maybe w- is a classical parameter, the sq- the variance between the um FFT of the signal and the small spectrum of time we - after the um mel filter bank.
me013: Uh-huh.
fn002: And, well, is more or less robust. Is good for clean speech. Is quite good
me013: Huh?
fn002: for noisy speech.
me013: Mm-hmm.
fn002: but um we must to have bigger statistic with TIMIT, and is not ready yet
me013: Mm-hmm. Yeah.
fn002: to use on, well, I don't know.
me013: Yeah.
mn007: Yeah. So, basically we wa- want to look at something like the ex- the ex- excitation signal and -
me013: Right.
fn002: Mm-hmm.
mn007: which are the variance of it and -
fn002: I have here. I have here
mn007: Mmm.
fn002: for one signal, for one frame.
me013: Yeah.
fn002: The - the mix of the two, noise and unnoise, and the signal is this.
me013: Uh-huh.
fn002: Clean, and this noise.
me013: Uh.
fn002: These are the two - the mixed, the big signal is for clean.
me013: Well, I'm s- uh - There's - None of these axes are labeled, so I don't know what this - What's this axis?
fn002: Uh this is uh - this axis is nnn, "frame".
me013: Frame.
fn002: Mm-hmm.
me013: And what's th- what this?
fn002: Uh, this is uh energy, log-energy of the spectrum. Of the - No, this is the variance, the difference between the spectrum of the signal and FFT of each frame of the signal and this mouth spectrum of time after the f-
me013: For this one.
fn002: may fit
me013: For the noi-
fn002: for the two, this big, to here, they are to signal. This is for clean and this is for noise.
me013: Oh. There's two things on the same graph.
fn002: Yeah. I don't know. I - I think that I have d- another graph, but I'm not sure.
mn007: Yeah.
me013: So w- which is clean and which is noise?
mn007: I think the lower one is noise.
fn002: The lower is noise and the height is clean.
me013: OK. So it's harder to distinguish
fn002: It's height.
me013: but it - but it g- with noise of course but - but -
mn007: Yeah.
fn002: Oh. I must to have. Pity, but I don't have
me013: Uh.
fn002: two different
me013: And presumably when there's a - a -
mn007: So this should the - the - the t- voiced
me013: Uh-huh.
fn002: Yeah, it is the height
mn007: portions.
fn002: is voiced portion.
mn007: The p- the peaks should be voiced portion.
fn002: And this is the noise portion.
me013: Uh-huh.
fn002: And this is more or less like this. But I meant to have see @@ two - two the picture.
me013: Yeah. Yeah.
fn002: This is, for example, for one frame.
me013: Yeah
fn002: the - the spectrum of the signal. And this is the small version of the spectrum after ML mel filter bank.
me013: Yeah. And this is the difference?
fn002: And this is I don't know. This is not the different. This is trying to obtain with LPC model the spectrum but using Matlab without going factor and s-
me013: No pre-emphasis? Yeah.
fn002: Not pre-emphasis. Nothing.
me013: Yeah so it's - doesn't do too well there.
fn002: And the - I think that this is good. This is quite similar. this is - this is another frame. ho- how I obtained the envelope, this envelope, with the mel filter bank.
me013: Right. So now I wonder - I mean, do you want to - I know you want to get at something orthogonal from what you get with the smooth spectrum Um. But if you were to really try and get a voiced-unvoiced, do you - do you want to totally ignore that? I mean, do you - do you - I mean, clearly a - a very big - very big cues for voiced-unvoiced come from uh spectral slope and so on, right?
mn007: Mm-hmm.
me013: Um.
mn007: Yeah. Well, this would be - this would be perhaps an additional parameter, simply isn't -
me013: Yeah. I see.
mn007: Yeah.
fn002: Yeah because when did noise
mn007: Uh.
fn002: clear in these section is clear
me013: Mm-hmm.
fn002: if s- @@ val- value is indicative that is a voice frame and it's low values @@
me013: Yeah. Yeah. Well, you probably want - I mean, certainly if you want to do good voiced-unvoiced detection, you need a few features. Each - each feature is by itself not enough. But, you know, people look at - at slope and uh
mn007: Mmm.
me013: first auto-correlation coefficient, divided by power. Or - or uh um there's uh - I guess we prob- probably don't have enough computation to do a simple pitch detector or something? I mean with a pitch detector you could have a -
mn007: Mmm.
me013: have a - an estimate of - of what the - Uh. Or maybe you could you just do it going through the P FFT's figuring out some um probable um harmonic structure. Right. And - and uh.
mn007: Mmm.
fn002: you have read up and - you have a paper, the paper that you s- give me yesterday.
mn007: Oh, yeah. But -
fn002: they say that yesterday they are some problem
mn007: Yeah, but it's not - it's, yeah, it's - it's another problem. Yeah
fn002: and the - Is another problem.
mn007: Um. Yeah, there is th- this fact actually. If you look at this um spectrum,
me013: Yeah.
mn007: What's this again? Is it the mel-filters?
fn002: Yeah like this. Of kind like this.
mn007: Yeah. OK. So the envelope here is the output of the mel-filters
me013: Mm-hmm.
mn007: and what we clearly see is that in some cases, and it clearly appears here, and the - the harmonics are resolved by the f- Well, there are still appear after mel-filtering,
me013: Mm-hmm.
mn007: and it happens for high pitched voice because the width of the lower frequency mel-filters is sometimes even smaller than the pitch.
me013: Yeah.
mn007: It's around one hundred, one hundred and fifty hertz
me013: Right.
mn007: Nnn. And so what happens is that this uh, add additional variability to this envelope and
me013: Yeah.
mn007: um so we were thinking to modify the mel-spectrum to have something that - that's smoother on low frequencies.
me013: That's as - as a separate thing. Yeah.
mn007: i- Yeah. This is a separate thing.
fn002: Yeah.
me013: Separate thing? Yeah.
mn007: And.
me013: Yeah. Maybe so. Um. Yeah. So, what - Yeah. What I was talking about was just, starting with the FFT you could - you could uh do a very rough thing to estimate - estimate uh pitch.
mn007: Yeah. Mm-hmm.
me013: And uh uh, given - you know, given that, uh you could uh uh come up with some kind of estimate of how much of the low frequency energy was - was explained by - by uh
mn007: Mm-hmm.
me013: uh those harmonics. Uh. It's uh a variant on what you're s- what you're doing . The - I mean, the - the the mel does give a smooth thing. But as you say it's not that smooth here. And - and so if you - if you just you know subtracted off uh your guess of the harmonics then something like this would end up with quite a bit lower energy in the first fifteen hundred hertz or so and -
mn007: Mm-hmm.
me013: and our first kilohertz, even. And um if was uh noisy, the proportion that it would go down would be if it was - if it was unvoiced or something.
mn007: Mm-hmm.
me013: So you oughta be able to pick out voiced segments. At least it should be another - another cue.
mn007: Mm-hmm.
me013: So. Anyway. OK? That's what's going on. Uh. What's up with you?
me006: Um our t- I went to talk with uh Mike Jordan this - this week
me013: Mm-hmm.
me006: um and uh shared with him the ideas about um extending the Larry Saul work and um I asked him some questions about factorial HMMs so like later down the line when we've come up with these - these feature detectors, how do we - how do we uh you know, uh model the time series that - that happens um and and we talked a little bit about factorial HMMs and how um when you're doing inference - or w- when you're doing recognition, there's like simple Viterbi stuff that you can do for - for these HMMs and the uh - the great advantages that um a lot of times the factorial HMMs don't um don't over- alert the problem there they have a limited number of parameters and they focus directly on - on uh the sub-problems at hand so you can imagine um five or so parallel um features um transitioning independently and then at the end you - you uh couple these factorial HMMs with uh - with uh undirected links um based on -
me013: Hmm.
me006: based on some more data. So he - he seemed - he seemed like really interested in - in um - in this and said - said this is - this is something very do-able and can learn a lot and um yeah, I've just been continue reading um about certain things.
me013: Mm-hmm.
me006: um thinking of maybe using um um m- modulation spectrum stuff to um - as features um also in the - in the sub-bands because
me013: Mm-hmm.
me006: it seems like the modulation um spectrum tells you a lot about the intelligibility of - of certain um words and stuff So, um. Yeah. Just that's about it.
me013: OK.
me026: OK. And um so I've been looking at Avendano's work and um uh I'll try to write up in my next stat- status report a nice description of what he's doing, but it's - it's an approach to deal with reverberation or that - the aspect of his work that I'm interested in the idea is that um normally an- analysis frames are um too short to encompass reverberation effects um in full. You miss most of the reverberation tail in a ten millisecond window and so you - you'd like it to be that um the reverberation responses um simply convolved um in, but it's not really with these ten millisecond frames cuz you j- But if you take, say, a two millisecond um window - I'm sorry a two second window then in a room like this, most of the reverberation response is included in the window and the - then it um then things are l- more linear. It is - it is more like the reverberation response is simply c- convolved and um - and you can use channel normalization techniques like uh in his thesis he's assuming that the reverberation response is fixed. He just does um mean subtraction, which is like removing the DC component of the modulation spectrum and that's supposed to d- um deal - uh deal pretty well with the um reverberation and um the neat thing is you can't take these two second frames and feed them to a speech recognizer um so he does this um method training trading the um the spectral resolution for time resolution and um come ca- uh synthesizes a new representation which is with say ten second frames but a lower s- um frequency resolution. So I don't really know the theory. I guess it's - these are called "time frequency representations" and h- he's making the - the time sh- um finer grained and the frequency resolution um less fine grained.
mn007: Mm-hmm.
me026: s- so I'm - I guess my first stab actually in continuing his work is to um re-implement this - this thing which um changes the time and frequency resolutions cuz he doesn't have code for me. So that that'll take some reading about the theory. I don't really know the theory.
mn007: Mm-hmm.
me026: Oh, and um, another f- first step is um, so the - the way I want to extend his work is make it able to deal with a time varying reverberation response um and um we don't really know how fast the um - the reverberation response is varying the Meeting Recorder data um so um we - we have this um block least squares um imp- echo canceller implementation and um I want to try finding the - the response, say, between a near mike and the table mike for someone using the echo canceller and looking at the echo canceller taps and then see how fast that varies from block to block. That should give an idea of how fast the reverberation response is changing.
mn007: Mm-hmm. Mm-hmm.
me013: OK. Um. I think we're sort of done.
mn007: Yeah.
me013: So let's read our digits and go home.
me026: Um. S- so um y- you do - I think you read some of the - the zeros as O's and some as zeros.
me013: Yeah.
me026: Is there a particular way we're supposed to read them?
mn007: There are only zeros here. Well.
me013: No. "O" - "O" - "O" and "zero" are two ways that we say that digit.
mn007: Eee. Yeah.
me013: So it's -
mn007: But -
me006: Ha!
me013: so it's - i-
mn007: Perhaps in the sheets there should be another sign for the - if we want to - the - the guy to say "O" or
me013: No. I mean. I think people will do what they say. It's OK .
mn007: It's - Yeah. OK.
me026: Alright.
me013: I mean in digit recognition we've done before, you have - you have two pronunciations for that value, "O" and "zero".
me026: OK.
mn007: But it's perhaps more difficult for the people to prepare the database then, if - because here you only have zeros and - and people pronounce "O" or zero -
me013: No, they just write - they - they write down OH. or they write down ZERO a- and they - and they each have their own pronunciation.
mn007: Yeah but if the sh- the sheet was prepared with a different sign for the "O".
me013: But people wouldn't know what that wa- I mean there is no convention for it.
mn007: OK. Yeah. OK.
me013: See. I mean, you'd have to tell them "OK when we write this, say it tha-", you know, and you just - They just want people to read the digits as you ordinarily would and - and people
mn007: Mm-hmm. Yeah. Yep.
me013: say it different ways.
me026: OK. Is this a change from the last batch of - of um forms? Because in the last batch it was spelled out which one you should read.
mn007: Yeah, it was orthographic, so.
me013: Yes. That's right. It was - it was spelled out, and they decided they wanted to get at more the way people would really say things. That's also why they're - they're bunched together in these different groups. So - so it's - Yeah. So it's - it's - Everything's fine.
me026: Oh. OK. OK. OK.
me013: OK. Actually, let me just s- since - since you brought it up, I was just - it was hard not to be self-conscious about that when it after we - since we just discussed it. But I realized that - that um when I'm talking on the phone, certainly, and - and saying these numbers, I almost always say zero. And uh - cuz - because uh i- it's two syllables. It's - it's more likely they'll understand what I said. So that - that - that's the habit I'm in, but some people say "O" and -
me006: Yeah I normally say "O" cuz it's easier to say.
me013: Yeah it's shorter. Yeah. So it's - So. So uh. Now, don't think about it.
me006: "O" Oh, no!
me013: OK. We're done.