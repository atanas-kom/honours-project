me013: Uh, is it the twenty-fourth? Yeah.
me018: now we're on.
mn052: Uh Chuck, is the mike type wireless -
me018: Yes.
mn052: wireless headset? OK.
me018: Yes.
me013: Yeah.
me018: For you it is.
me013: Yeah. We uh - we abandoned the lapel because they sort of were not too - not too hot, not too cold, they were - you know, they were uh, far enough away that you got more background noise, uh, and uh - and so forth but they weren't so close that they got quite the - you know, the really good - No, th-
mn052: Uh-huh. OK.
me013: they - I mean they didn't - Wait a minute. I'm saying that wrong. They were not so far away that they were really good representative distant mikes, but on the other hand they were not so close that they got rid of all the interference. So it was no - didn't seem to be a good point to them.
mn052: Uh-huh.
me013: On the other hand if you only had to have one mike in some ways you could argue the lapel was a good choice, precisely because it's in the middle.
mn052: Yeah, yeah.
me013: There's uh, some kinds of junk that you get with these things that you don't get with the lapel uh, little mouth clicks and breaths and so forth are worse with these than with the lapel, but given the choice we - there seemed to be very strong opinions for uh, getting rid of lapels. So,
mn052: The mike number is -
me018: Uh, your mike number's written on the back of that unit there.
mn052: Oh yeah. One.
me018: And then the channel number's usually one less than that.
mn052: Oh, OK.
me018: It- it's one less than what's written on the back of your - yeah.
mn052: OK. OK. OK.
me018: So you should be zero, actually.
mn052: Hello? Yeah.
me018: For your uh, channel number.
mn052: Yep, yep.
me013: And you should do a lot of talking so we get a lot more of your pronunciations. no, they don't - don't have a - have any Indian pronunciations.
me018: So what we usually do is um, we typically will have our meetings and then at the end of the meetings we'll read the digits. Everybody goes around and reads the digits on the - the bottom of their forms.
me013: Yeah. Session R-
mn007: Rnineteen?
me013: Rnineteen.
mn052: OK.
me018: Yeah. We're - This is session Rnineteen.
me013: If you say so. O K. Do we have anything like an agenda? What's going on? Um. I guess um. So. One thing -
me018: Sunil's here for the summer?
me013: Sunil's here for the summer, right. Um, so, one thing is to talk about a kick off meeting maybe uh, and then just uh, I guess uh, progress reports individually, and then uh, plans for where we go between now and then, pretty much. Um.
me018: I could say a few words about um, some of the uh, compute stuff that's happening around here, so that people in the group know.
me013: Mm-hmm. OK. Why don't you start with that? That's sort of - Yeah?
me018: OK. We um - So we just put in an order for about twelve new machines, uh, to use as sort of a compute farm. And um, uh, we ordered uh, SUN-Blade-one-hundreds, and um, I'm not sure exactly how long it'll take for those to come in, but, uh, in addition, we're running - So the plan for using these is, uh, we're running Pmake and Customs here and Andreas has sort of gotten that all uh, fixed up and up to speed. And he's got a number of little utilities that make it very easy to um, run things using Pmake and Customs. You don't actually have to write Pmake scripts and things like that. The simplest thing - And I can send an email around or, maybe I should do an FAQ on the web site about it or something. Um, there's a c-
me013: How about an email that points to the FAQ, you know what I'm saying? so that you can - Yeah.
me018: Yeah, yeah. Uh, there's a command, uh, that you can use called " run command". "Run dash command", "run hyphen command". And, if you say that and then some job that you want to execute, uh, it will find the fastest currently available machine, and export your job to that machine, and uh - and run it there and it'll duplicate your environment. So you can try this as a simple test with uh, the L S command. So you can say "run dash command L S ", and, um, it'll actually export that LS command to some machine in the institute, and um, do an LS on your current directory. So, substitute LS for whatever command you want to run, and um - And that's a simple way to get started using - using this. And, so, soon, when we get all the new machines up, um, e- then we'll have lots more compute to use. Now th- one of the nice things is that uh, each machine that's part of the Pmake and Customs network has attributes associated with it. Uh, attributes like how much memory the machine has, what its speed is, what its operating system, and when you use something like " run command", you can specify those attributes for your program. For example if you only want your thing to run under Linux, you can give it the Linux attribute, and then it will find the fastest available Linux machine and run it on that. So. You can control where your jobs go, to a certain extent, all the way down to an individual machine. Each machine has an attribute which is the name of itself. So you can give that as an attribute and it'll only run on that. If there's already a job running, on some machine that you're trying to select, your job will get queued up, and then when that resource, that machine becomes available, your job will get exported there. So, there's a lot of nice features to it and it kinda helps to balance the load of the machines and uh, right now Andreas and I have been the main ones using it and we're - Uh. The SRI recognizer has all this Pmake customs stuff built into it. So.
me013: So as I understand, you know , he's using all the machines and you're using all the machines,
me018: Yeah. Exactly.
me013: is the rough division of -
me018: Yeah, you know, I - I sort of got started using the recognizer just recently and uh, uh I fired off a training job, and then I fired off a recognition job and I get this email about midnight from Andreas saying, "uh, are you running two trainings simultaneously s- my m- my jobs are not getting run." So I had to back off a little bit. But, soon as we get some more machines then uh - then we'll have more compute available. So, um, that's just a quick update about what we've got. So.
me006: Um, I have - I have a question about the uh, parallelization?
me018: Mm-hmm.
me006: So, um, let's say I have like, a thousand little - little jobs to do?
me018: Mm-hmm.
me006: Um, how do I do it with "run command"? I mean do -
me018: You could write a script uh, which called run command on each sub-job right? But you probably
me006: Uh-huh. A thousand times? OK.
me018: wanna be careful with that because um, you don't wanna saturate the network. Uh, so, um, you know, you should - you should probably not run more than, say ten jobs yourself at any one time, uh, just because then it would
me006: Oh, too much file transfer and stuff.
me018: keep other people - Well it's not that so much as that, you know, e- with - if everybody ran fifty jobs at once then it would just bring everything to a halt and, you know, people's jobs would get delayed, so it's sort of a sharing thing.
me006: OK.
me018: Um, so you should try to limit it to somet- sometim- some number around ten jobs at a time. Um. So if you had a script for example that had a thousand things it needed to run, um, you'd somehow need to put some logic in there if you were gonna use "run command", uh, to only have ten of those going at a time. And uh, then, when one of those finished you'd fire off another one. Um,
me013: I remember I - I forget whether it was when the Rutgers or - or Hopkins workshop, I remember one of the workshops I was at there were - everybody was real excited cuz they got twenty-five machines and there was some kind of Pmake like thing that sit- sent things out.
me018: Mm-hmm. Mm-hmm. Mm-hmm.
me013: So all twenty-five people were sending things to all twenty-five machines and and things were a lot less efficient than if you'd just use your own machine. as I recall, but. Yeah.
me018: Yeah. Yeah. Yep. Yeah, exactly. Yeah, you have to be a little bit careful.
mn007: Hmm.
me018: Um, but uh, you can also - If you have that level of parallelization um, and you don't wanna have to worry about writing the logic in - in a Perl script to take care of that, you can use um, Pmake
me006: Just do Pmake. s-
me018: and - and you basically write a Make file that uh, you know your final job depends on these one thousand things, and when you run
me006: Mm-hmm.
me018: Pmake, uh, on your Make file, you can give it the dash capital J
me006: Mm-hmm.
me018: and - and then a number, and that number represents how many uh, machines to use at once.
me006: Right.
me018: And then it'll make sure that it never goes above that.
me006: Right. OK.
me018: So, I can get some documentation.
mn007: So it - it's - it's not systematically queued. I mean all the jobs are running. If you launch twenty jobs, they are all running.
me018: It depends. If you - "Run command", that I mentioned before, is - doesn't know about other things that you might be running.
mn007: Alright. Uh-huh.
me018: So, it would be possible to run a hundred run jobs at once,
mn007: Right.
me018: and they wouldn't know about each other. But if you use Pmake , then, it knows about all the jobs that it has to run
mn007: Mm-hmm.
me018: and it can control, uh, how many it runs simultaneously.
me013: So "run command" doesn't use Pmake, or - ?
me018: It uses "export" underlyingly. But, if you - i- It's meant to be run one job at a time? So you could fire off a thousand of those, and it doesn't know - any one of those doesn't know about the other ones that are running.
me013: So why would one use that rather than Pmake ?
me018: Well, if you have, um - Like, for example, uh if you didn't wanna write a Pmake script and you just had a, uh - an HTK training job that you know is gonna take uh, six hours to run, and somebody's using, uh, the machine you typically use, you can say "run command" and your HTK thing and it'll find another machine, the fastest currently available machine and - and run your job there.
me013: Now, does it have the same sort of behavior as Pmake, which is that, you know, if you run something on somebody's machine and they come in and hit a key then it -
me018: Yes. Yeah, there are um - Right. So some of the machines at the institute, um, have this attribute called " no evict". And if you specify that, in - in one of your attribute lines, then it'll go to a machine which your job won't be evicted from.
me013: Mm-hmm.
me018: But, the machines that don't have that attribute, if a job gets fired up on that, which could be somebody's desktop machine, and - and they were at lunch, they come back from lunch and they start typing on the console,
me013: Mm-hmm.
me018: then your machine will get evicted - your job will get evicted from their machine and be restarted on another machine. Automatically. So - which can cause you to lose time, right? If you had a two hour job, and it got halfway through and then somebody came back to their machine and it got evicted. So. If you don't want your job to run on a machine where it could be evicted, then you give it the minus - the attribute, you know, "no evict", and it'll pick a machine that it can't be evicted from. So.
me013: Um, what - what about - I remember always used to be an issue, maybe it's not anymore, that if you - if something required - if your machine required somebody hitting a key in order to evict things that are on it so you could work,
me018: Mm-hmm.
me013: but if you were logged into it from home? and you weren't hitting any keys? cuz you were, home?
me018: Yeah, I - I'm not sure how that works. Uh, it seems like Andreas did something for that. Um.
me013: Yeah. Hmm. OK. We can ask him sometime.
me018: But - Yeah. I don't know whether it monitors the keyboard or actually looks at the console TTY, so maybe if you echoed something to the you know, dev - dev console or something. Hmm?
me013: You probably wouldn't ordinarily, though. Yeah. Right? You probably wouldn't ordinarily. I mean you sort of - you're at home and you're trying to log in, and it takes forever to even log you in, and you probably go, "screw this", and - You know. Yeah.
me018: Yeah, yeah. Yeah. Yeah, so, um, yeah. I - I can - I'm not sure about that one. But uh.
me013: yeah. OK.
mn052: Uh, I need a little orientation about this environment and uh scr- s- how to run some jobs here because I never d- did anything so far with this X emissions So,
me018: OK.
mn052: I think maybe I'll ask you after the meeting.
me018: Um. Yeah. Yeah, and - and also uh, Stephane's a - a really good resource for that if you can't
mn052: Yeah, yeah, yeah. Yep. OK, sure @@
me018: find me.
mn007: Mmm.
me018: Especially with regard to the Aurora stuff. He - he knows that stuff better than I do.
mn052: OK.
me013: OK. Well, why don't we uh, uh, Sunil since you're haven't - haven't been at one of these yet, why don't yo- you tell us what's - what's up with you? Wh- what you've been up to, hopefully.
mn052: Um. Yeah. So, uh, shall I start from - Well I don't know how may I - how - OK. Uh, I think I'll start from the post uh Aurora submission maybe.
me013: Yeah.
mn052: Uh, yeah, after the submission the - what I've been working on mainly was to take - take other s- submissions and then over their system, what they submitted, because we didn't have any speech enhancement system in - in ours. So - So I tried uh, And u- First I tried just LDA. And then I found that uh, I mean, if - if I combine it with LDA, it gives @@ improvement over theirs. Uh -
me018: Are y- are you saying LDA?
mn052: Yeah.
me018: LDA. OK.
mn052: Yeah. So, just - just the LDA filters. I just plug in - I just take the cepstral coefficients coming from their system and then plug in LDA on top of that.
me018: Mm-hmm.
mn052: But the LDA filter that I used was different from what we submitted in the proposal. What I did was I took the LDA filter's design using clean speech, uh, mainly because the speech is already cleaned up after the enhancement so, instead of using this, uh, narrow - narrow band LDA filter that we submitted uh, I got new filters. So that seems to be giving - uh, improving over their uh, system. Slightly. But, not very significantly. And uh, that was uh, showing any improvement over - final - by plugging in an LDA. And uh, so then after - after that I - I added uh, on-line normalization also on top of that. And that - there - there also I n- I found that I have to make some changes to their time constant that I used because th- it has a - a mean and variance update time constant and - which is not suitable for the enhanced speech, and whatever we try it on with proposal-one. But um, I didn't - I didn't play with that time constant a lot, I just t- g- I just found that I have to reduce the value - I mean, I have to increase the time constant, or reduce the value of the update value. That's all I found So I have to . Uh, Yeah. And uh, uh, the other - other thing what I tried was, I just um, uh, took the baseline and then ran it with the endpoint inf- uh th- information, just the Aurora baseline, to see that how much the baseline itself improves by just supplying the information of the - I mean the w- speech and nonspeech. And uh, I found that the baseline itself improves by twenty-two percent by just giving the wuh- .
me013: Uh, can you back up a second, I - I - I missed something, uh, I guess my mind wandered. Ad- ad- When you added the on-line normalization and so forth, uh, uh things got better again? or is it? Did it not?
mn052: Yeah. No. No. No, things didn't get better with the same time constant that we used.
me013: No, no. With a different time constant.
mn052: With the different time constant I found that - I mean, I didn't get an improvement over not using on-line normalization,
me013: Oh. No you didn't, OK.
mn052: because I - I found that I would have change the value of the update factor. But I didn't play it with
me013: Yeah.
mn052: play - play quite a bit to make it better than.
me013: OK.
mn052: So, it's still not - I mean, the on-line normalization didn't give me any improvement.
me013: OK.
mn052: And uh,
me013: OK.
mn052: so, oh yeah So I just stopped there with the uh, speech enhancement. The - the other thing what I tried was the - adding the uh, endpoint information to the baseline and that itself gives like twenty-two percent because the - the second - the new phase is going to be with the endpointed speech. And just to get a feel of how much the baseline itself is going to change by adding this endpoint information, I just,
me013: Hmm.
mn052: uh, use -
me018: So people won't even have to worry about, uh, doing speech-nonspeech then.
mn052: Yeah that's, that's what the feeling is like. They're going to give the endpoint information.
me018: Mmm. I see.
me013: G- I guess the issue is that people do that anyway, everybody does that, and they wanted to see,
mn052: Yeah.
me013: given that you're doing that, what - what are the best features that you should use.
me018: Yeah, I see.
mn052: So,
me013: I mean clearly they're interact. So I don't know that I entirely agree with it. But - but it might be uh - In some ways it might be better t- to -
me018: Yeah.
me013: rather than giving the endpoints, to have a standard
me018: Mm-hmm.
me013: that everybody uses and then interacts with. But, you know. It's - it's still someth- reasonable.
me018: So, are people supposed to assume that there is uh - Are - are people not supposed to use any speech outside of those endpoints?
mn052: Uh -
me018: Or can you then
mn052: No. No. That i- I -
me018: use speech outside of it for estimating background noise and things?
mn052: Yeah. Yeah, yeah, exactly. I guess that is - that is where the consensus is. Like y- you will - you will - You'll be given the information about the beginning and the end of speech but the whole speech is available to you.
me018: OK.
me013: So it should make the spectral subtraction style things work even better,
mn052: So.
me013: because you don't have the mistakes in it.
mn052: Yeah.
me013: Yeah? OK.
mn052: Yeah. So - So that - that - The baseline itself - I mean, it improves by twenty-two percent. I found that in s- one of the SpeechDat-Car cases, that like, the Spanish one improves by just fifty percent by just putting the endpoint.
me018: Wow.
mn052: w- I mean you don't need any further speech enhancement with fifty. So, uh,
me018: So the baseline itself improves by fifty percent.
mn052: Yeah, by fifty percent.
me013: Yeah. So it's g- it's gonna be harder to beat that actually. But - but -
me018: Wow. Yeah.
mn052: Yeah, so - so that is when uh, the - the qualification criteria was reduced from fifty percent to something like twenty-five percent for well-matched. And I think they have - they have actually changed their qualification c- criteria now. And uh, Yeah, I guess after that, I just went home f- I just had a vacation fo- for four weeks.
me013: OK. No, that's - that's - that's a good - good update.
mn052: Uh. Ye- Yeah, and I - I came back and I started working on uh, some other speech enhancement algorithm. I mean, so - I - from the submission what I found that people have tried spectral subtraction and Wiener filtering. These are the main uh, approaches where people have tried, so just to -
me013: Yeah.
mn052: just to fill the space with some f- few more speech enhancement algorithms to see whether it improves a lot , I - I've been working on this uh, signal subspace approach for speech enhancement where you take the noisy signal and then decomposing the signal s- and the noise subspace and then try to estimate the clean speech from the signal plus noise subspace.
me013: Mm-hmm.
mn052: And - So, I've been actually running some s- So far I've been trying it only on Matlab. I have to -
me013: Yeah.
mn052: to - to test whether it works first or not and then I'll p- port it to C and I'll update it with the repository once I find it- it giving any some positive result. So, yeah.
me013: S- So you s- you So you said one thing I want to jump on for a second. So - so now you're - you're getting tuned into the repository thing that he has here and -
mn052: Yeah.
me013: so we- we'll have a single place where the stuff is.
mn052: Yep. Yeah.
me013: Cool. Um, so maybe uh, just briefly, you could remind us about the related experiments. Cuz you did some stuff that you talked about last week, I guess?
mn007: Mm-hmm.
me013: Um, where you were also combining something - both of you I guess were both combining something from the uh,
mn007: Right.
me013: French Telecom system with the u- uh - I - I don't know whether it was system one or system two, or - ?
mn007: Mm-hmm. It was system one. So we -
me013: OK.
mn007: The main thing that we did is just to take the spectral subtraction from the France Telecom, which provide us some speech samples that are uh, with noise removed.
me013: So I let me - let me just stop you there. So then, one distinction is that uh, you were taking the actual France Telecom features and then applying something to -
mn052: Uh, no there is a slight different. Uh I mean, which are extracted at the handset
me013: Yeah.
mn052: because they had another back-end blind equalization -
me013: Yeah.
mn052: Yeah. Yeah.
me013: But that's what I mean. But u- u- Sorry, yeah, I'm not being - I'm not being clear.
mn052: Yeah. Yeah.
me013: What I meant was you had something like cepstra or something, right?
mn052: Yeah, yeah, yeah, yeah.
me013: And so one difference is that, I guess you were taking spectra.
mn052: The speech.
fn002: Yeah.
mn007: Yeah. But I guess it's the s- exactly the same thing because on the heads- uh, handset they just applied this Wiener filter and then compute cepstral features, right? or - ?
mn052: Yeah, the cepstral f- The difference is like - There may be a slight difference in the way - because they use exactly the baseline system for converting the cepstrum once you have the speech.
mn007: Right.
mn052: I mean, if we are using our own code for th- I mean that - that could be the only difference. I mean, there is no other difference. Yeah.
mn007: Mm-hmm.
me013: But you got some sort of different result. So I'm trying to understand it. But uh,
mn007: Yeah, well I think we should uh, have a table with all the result because I don't know I uh, I don't exactly know what are your results? But,
me013: I th-
mn052: OK. OK.
mn007: Mmm. Yeah, but so we did this, and another difference I guess is that we just applied uh, proposal-one system after this without - well, with our modification to reduce the delay of the - the LDA filters, and
mn052: Uh-huh.
mn007: Well there are slight modifications, but it was the full proposal-one. In your case, if you tried
fn002: And the filter -
mn052: Only LDA.
mn007: just putting LDA, then maybe on-line normalization - ?
mn052: Yeah. Af- I - after that I added on-line normalization, yeah.
mn007: Mm-hmm. So we just tried directly to - to just, keep the system as it was and, um, when we plug the spectral subtraction it improves uh, signif- significantly. Um, but, what seems clear also is that we have to retune the time constants of the on-line normalization. Because if we keep the value that was submitted
mn052: Yeah, yeah. Yeah.
mn007: uh, it doesn't help at all. You can remove on-line normalization, or put it, it doesn't change anything. Uh, uh, as long as you have the spectral subtraction. But, you can still find some kind of optimum somewhere, and we don't know where exactly but,
mn052: Yeah.
mn007: uh.
mn052: Yeah, I assume.
me013: So it sounds like you should look at some tables of results or something and see where i-
mn007: Right. Yeah.
mn052: Yeah.
me013: where the -
mn007: Mm-hmm.
me013: where they were different and what we can learn from it.
mn007: Mm-hmm.
mn052: without any change.
mn007: Yeah.
fn002: But it's -
mn052: OK.
mn007: Well,
fn002: It's the new.
mn007: with - with -
fn002: The new.
mn007: with changes, because we change it the system to have -
mn052: with Oh yeah, I mean the - the new LDA filters. I mean - OK.
fn002: The new.
mn007: Yeah. LDA filters. There are other things that we finally were shown to improve also like, the sixty-four hertz cut-off.
fn002: Mm-hmm.
mn052: Mm-hmm.
mn007: w- Uh, it doesn't seem to hurt on TI-digits, finally.
mn052: OK.
mn007: Maybe because of other changes.
mn052: OK.
mn007: Um, well there are some minor changes, yeah.
mn052: Mm-hmm.
mn007: And, right now if we look at the results, it's, um, always better than - it seems always better than France Telecom for mismatch and high-mismatch. And it's still slightly worse for well-matched. Um,
fn002: But
mn007: but this is not significant. But, the problem is that it's not significant, but if you put this in the, mmm, uh, spreadsheet, it's still worse. Even with very minor - uh, even if it's only slightly worse for well-matched.
me013: Mm-hmm.
mn007: And significantly better for HM. Uh, but, well. I don't think it's importa- important because when they will change their metric, uh, uh, mainly because of uh, when you p- you plug the um, frame dropping in the baseline system, it will improve a lot HM, and MM, so,
mn052: Yeah.
mn007: um, I guess what will happen - I don't know what will happen. But, the different contribution, I think, for the different test set will be more even.
mn052: Because the - your improvement on HM and MM will also go down significantly in the spreadsheet so. But the the well-matched may still - I mean the well-matched may be the one which is least affected by adding the
mn007: Mm-hmm.
mn052: endpoint information.
me013: Right.
mn052: Yeah. So the - the MM -
mn007: Mm-hmm.
mn052: MM and HM are going to be v- hugely affected by it. Yeah.
mn007: Yeah, so um, yeah.
mn052: Yeah. But they d- the - everything I mean is like, but there that's how they reduce - why they reduce the qualification to twenty-five percent or some - something on.
mn007: Mm-hmm.
me013: But are they changing the weighting?
mn052: Uh, no, I guess they are going ahead with the same weighting.
mn007: Yeah.
mn052: Yeah. So there's nothing on -
me013: I don't understand that. I guess I - I haven't been part of the
mn052: Yeah.
me013: discussion, so, um, it seems to me that the well-matched condition is gonna be unusual, in this case. Unusual.
mn052: Usual. Uh-huh.
me013: Because, um, you don't actually have good matches ordinarily for what any @@ - particular person's car is like, or uh,
mn052: Mmm. Mmm.
me013: It seems like something like the middle one is - is more
mn052: Hmm.
me013: natural. So I don't know why the well-matched is
mn052: Right .
me013: uh -
mn007: Mm-hmm.
mn052: Yeah, but actually the well - well the well-matched um, uh, I mean the - the well-matched condition is not like, uh, the one in TI-digits where uh, you have all the training, uh, conditions exactly like replicated in the testing condition also. It's like, this is not calibrated by SNR or something. The well-matched has also some - some mismatch in that which is other than the -
me013: The well wa- matched has mismatch?
mn052: has - has also some slight mismatches, unlike the TI-digits where it's like perfectly matched because it's artificially added noise.
me018: Perfect to match.
me013: Yeah. Yeah.
mn052: But this is natural recording.
me013: So remind me of what well-matched meant? You've told me many times.
mn052: The - the well-matched is like - the - the well-matched is defined like it's seventy percent of the whole database is used for training and thirty percent for testing.
mn007: Yeah. Well, so it means that if the database is large enough, it's matched. Because
mn052: It's - it's - OK, it's -
mn007: it
me013: Yeah.
mn007: in each set you have a range of conditions - Well -
me013: Right. So, I mean, yeah, unless they deliberately chose it to be different, which they didn't because they want it to be well-matched, it is pretty much - You know, so it's - so it's sort of saying if you -
me018: It's - it's not guaranteed though.
mn052: Yeah.
me013: Uh, it's not guaranteed. Right. Right.
mn052: Yeah.
mn007: Mm-hmm.
mn052: Yeah because the m- the main - major reason for the m- the main mismatch is coming from the amount of noise and the silence frames and all those present in the database actually.
me013: Again, if you have enough - if you have enough -
mn052: No- yeah, yeah. Yeah.
me013: So it's sort of i- i- it's sort of saying OK, so you - much as you train your dictation machine for talking into your computer, um, you - you have a car, and so you drive it around a bunch and - and record noise conditions, or something, and then - I don't think that's very realistic, I mean I th- I - I you know, so I -
mn052: Mm-hmm.
me013: I - I - you know, I guess they're saying that if you were a company that was selling the stuff commercially, that you would have a bunch of people driving around in a bunch of cars, and - and you would have something that was roughly similar and maybe that's the argument, but I'm not sure I buy it, so.
mn052: Yeah, yeah, yeah.
me013: Uh, So What else is going on?
mn007: Mmm. You- Yeah. We are playing - we are also playing, trying to put other spectral subtraction mmm, in the code. Um, it would be a very simple spectral subtraction, on the um, mel energies which I already tested but without the um frame dropping actually, and I think it's important to have frame dropping
me018: Is it - is spectral subtraction typically done on the - after the mel, uh, scaling or is it done on the FFT bins?
mn007: if you use spectral subtraction. Um,
me018: Does it matter, or - ?
mn007: I d- I don't know. Well, it's both - both uh, cases can i-
me018: Oh.
mn007: Yeah. So- some of the proposal, uh, we're doing this on the bin - on the FFT bins, others on the um, mel energies. You can do both, but I cannot tell you what's -
me018: Hmm.
mn007: which one might be better or -
me018: Hmm.
mn007: I -
mn052: I guess if you want to reconstruct the speech, it may be a good idea to do it on FFT bins.
mn007: I don't know.
me018: Mmm.
mn007: Yeah, but
mn052: But for speech recognition, it may not. I mean it may not be very different if you do it on mel warped or whether you do it on FFT.
me018: I see.
mn052: So you're going to do a linear weighting anyway after that. Well - Yeah?
me018: Hmm.
mn052: So, it may not be really a big different.
mn007: Well, it gives something different, but I don't know what are the, pros and cons of
mn052: It- I- Uh-huh.
mn007: both.
me013: Hmm.
mn052: So
me013: OK.
mn052: The other thing is like when you're putting in a speech enhancement technique, uh, is it like one stage speech enhancement? Because everybody seems to have a mod- two stages of speech enhancement in all the proposals, which is really giving them some improvement.
mn007: Yeah.
fn002: Mm-hmm.
mn007: Mm-hmm.
mn052: I mean they just do the same thing again once more.
me013: Mm-hmm.
mn052: And - So, there's something that is good about doing it - I mean, to cleaning it up once more.
mn007: Yeah, it might be. Yeah.
mn052: Yeah, so we can -
mn007: So maybe in my implementation I should also
mn052: Yeah.
mn007: try to inspire me from
mn052: That's what
mn007: this kind of thing and - Yeah.
me013: Well, the other thing would be to combine what you're doing. I mean maybe one or - one or the other of the things that you're doing would benefit from the other happening first.
mn052: That's wh- Yeah. So,
me013: Right, so he's doing a signal subspace thing, maybe it would work better if you'd already done some simple spectral subtraction, or maybe vi- maybe the other way around, you know?
mn007: Yeah, mm-hmm.
mn052: Yeah. So I 've been thinking about combining the Wiener filtering with signal subspace,
mn007: Mm-hmm.
mn052: I mean just to see all - some - some such permutation combination to see whether it really helps or not.
mn007: Mm-hmm. Mm-hmm. Mm-hmm. Yeah. Yeah.
me013: How is it - I - I guess I'm ignorant about this, how does - I mean, since Wiener filter also assumes that you're - that you're adding together the two signals, how is - how is that differ from signal subspace?
mn052: The signal subspace?
me013: Yeah.
mn052: The - The signal subspace approach has actually an in-built Wiener filtering in it.
me013: Oh, OK.
mn052: Yeah. It is like a KL transform followed by a Wiener filter. Is the signal is - is a signal substrate .
me013: Oh, oh, OK so the difference is the KL.
mn052: So, the - the different - the c- the - the advantage of combining two things is mainly coming from the signal subspace approach doesn't work very well if the SNR is very bad. It's -
me013: I see.
mn052: it works very poorly with the poor SNR conditions, and in colored noise.
me013: So essentially you could do simple spectral subtraction, followed by a KL transform, followed by a
mn052: Wiener filtering.
me013: Wiener filter.
mn052: It's a - it's a cascade of two s-
me013: Yeah, in general, you don't - that's right you don't wanna othorg- orthogonalize if the things are noisy. Actually. Um, that was something that uh, Herve and I were talking about with um, the multi-band stuff, that if you're converting things to from uh, bands, groups of bands into cepstral coef- you know, local sort of local cepstral coefficients that it's not that great to do it if it's noisy.
mn052: Mm-hmm. OK. Yeah.
me013: Uh,
mn052: So.
me013: so.
mn052: So that - that's one reason maybe we could combine s- some - something to improve SNR a little bit,
me013: Yeah.
mn052: first stage, and then do a something in the second stage which could take it further.
mn007: What was your point about - about colored noise there?
mn052: Oh, the colored noise uh -
mn007: Yeah.
mn052: the colored noise - the - the v- the signal subspace approach has - I mean, it - it actually depends on inverting the matrices. So it - it - ac- the covariance matrix of the noise.
mn007: Mm-hmm.
mn052: So if - if it is not positive definite, I mean it has a - it's - It doesn't behave very well if it is not positive definite ak- It works very well with white noise because we know for sure that it has a positive definite.
me013: So you should do spectral subtraction and then add noise.
mn052: So the way they get around is like they do an inverse filtering, first of the colo- colored noise and then make the noise white, and then finally when you reconstruct the speech back, you do this filtering again.
me013: Yeah. Yeah.
mn007: Yeah, right.
me013: I was only half kidding. I mean if you - sort of you do the s- spectral subtraction,
mn007: Yeah.
mn052: Yeah. Yeah.
me013: that also gets rid - and then you - then - then add a little bit l- noise - noise addition - I mean, that sort of what J - JRASTA does, in a way. If you look at what JRASTA doing essentially i- i- it's equivalent to sort of adding a little - adding a little noise,
mn052: Yeah. Huh? Uh-huh.
mn007: Uh-huh.
me013: in order to get rid of the effects of noise.
mn052: So.
mn007: Yeah.
me013: OK.
mn007: Uh, yeah. So there is this. And maybe we - well we find some people so that uh, agree to maybe work with us, and they have implementation of VTS techniques so it's um, Vector Taylor Series that are used to mmm, uh f- to model the transformation between clean cepstra and noisy cepstra. So. Well, if you take the standard model of channel plus noise,
me013: Mm-hmm.
mn007: uh, it's - it's a nonlinear eh- uh, transformation in the cepstral domain.
me013: Yes.
mn007: And uh, there is a way to approximate this using uh, first-order or second-order Taylor Series and it can be used for uh, getting rid of the noise and the channel effect.
me013: Who is doing this?
mn007: Uh w- working in the cepstral domain? So there is one guy in Grenada,
fn002: Yeah, in Grenada one of my friend.
mn007: and another in uh, Lucent that I met at ICASSP. uh,
me013: Who's the guy in Grenada?
fn002: Uh, Jose Carlos Segura.
me013: I don't know him .
mn052: This VTS has been proposed by CMU? Is it - is it the CMU? Yeah, yeah, OK. From C.
mn007: Mm-hmm.
fn002: Yeah, yeah, yeah. Originally the idea was from CMU.
mn007: Mm-hmm. Yeah.
me013: Uh-huh.
mn007: Well, it's again a different thing that could be tried. Um,
me013: Uh-huh.
mn007: Mmm, yeah.
me013: Yeah, so at any rate, you're looking general, uh, standing back from it, looking at ways to combine one form or another of uh, noise removal, uh, with - with these other things we have,
mn007: Mm-hmm.
me013: uh, looks like a worthy thing to - to do here.
mn007: Uh, yeah. But, yeah. But for sure there's required to - that requires to re-check everything else, and re-optimize
me013: Oh yeah.
mn007: the other things and, for sure the on-line normalization may be the LDA filter. Um,
me013: Well one of the - seems like one of the things to go through next week when Hari's here, cuz Hari'll have his own ideas
mn007: I -
me013: too - or I guess not next week, week and a half,
mn007: Uh-huh.
me013: uh, will be sort of go through these alternatives, what we've seen so far, and come up with some game plans. Um. You know. So, I mean one way would - he- Here are some alternate visions. I mean one would be, you look at a few things very quickly, you pick on something that looks like it's promising and then everybody works really hard on the same - different aspects of the same thing. Another thing would be to have t- to - to pick two pol- two plausible things, and - and you know, have t- sort of two working things for a while until we figure out what's better, and then, you know, uh,
mn007: Mm-hmm.
me013: but, w- um, uh, he'll have some ideas on that too.
mn052: The other thing is to, uh - Most of the speech enhancement techniques have reported results on small vocabulary tasks. But we - we going to address this Wall Street Journal in our next stage, which is also going to be a noisy task so s- very few people have reported something on using some continuous speech at all. So, there are some - I mean, I was looking at some literature on speech enhancement applied to large vocabulary tasks and spectral subtraction doesn't seems to be the thing to do for large vocabulary tasks. And it's - Always people have shown improvement with Wiener filtering and maybe subspace approach over spectral subtraction everywhere. But if we - if we have to use simple spectral subtraction, we may have to do some optimization to make it
me013: So they're making - there - Somebody's generating Wall Street Journal with additive - artificially added noise or something?
mn052: work @@ . Yeah, yeah.
me013: Sort of a - sort of like what they did with TI-digits, and? Yeah, OK.
mn052: Yeah. Yeah. I m- I guess Guenter Hirsch is in charge of that.
me013: OK.
mn052: Guenter Hirsch and TI. Maybe Roger - r- Roger, maybe in charge of.
me013: And then they're - they're uh, uh, generating HTK scripts to -
mn052: Yeah. Yeah, I don't know. There are - they have - there is no - I don't know if they are converging on HTK or are using some Mississippi State, yeah. I'm not sure about that.
me013: Mis- Mississippi State maybe, yeah. Yeah, so that'll be a little - little task in itself. Um, well we've -
mn052: Yeah.
me013: Yeah, it's true for the additive noise, y- artificially added noise we've always used small vocabulary too. But for n- there's been noisy speech this larv- large vocabulary that we've worked with in Broadcast News. So we- we did the Broadcast News evaluation and
mn052: Mm-hmm.
me013: some of the focus conditions were noisy and - and - But we - but we didn't do spectral subtraction. We were doing our funny stuff, right? We were doing multi- multi- uh, multi-stream and - and so forth.
mn052: It had additive n- Yeah.
me013: But it, you know, we di- stuff we did helped. I mean it, did something. So.
mn052: OK.
me013: Um, now we have this um, meeting data. You know, like the stuff we're recording right now, and -
mn052: Yeah. Yeah.
me013: and uh, that we have uh, for the - uh, the quote-unquote noisy data there is just - noisy and reverberant actually. It's the far field mike. And uh, we have uh, the digits that we do at the end of these things. And that's what most o- again, most of our work has been done with that, with - with uh, connected digits.
mn052: Uh-huh.
me013: Um, but uh, we have recognition now with some of the continuous speech, large vocabulary continuous speech, using Switchboard - uh, Switchboard recognizer,
mn052: Yeah. OK.
me013: uh, no training, from this, just - just plain using the Switchboard.
mn052: Oh. You just take the Switchboard trained - ? Yeah, yeah.
me013: That's - that's what we're doing, yeah. Now there are some adaptation though, that - that uh, Andreas has been playing with, but we're hop- uh, actually uh, Dave and I were just talking earlier today about maybe at some point not that distant future, trying some of the techniques
mn052: OK. Yeah. That's cool. OK.
me013: that we've talked about on, uh, some of the large vocabulary data. Um, I mean, I guess no one had done - yet done test one on the distant mike using uh, the SRI recognizer and, uh,
me018: I don't - not that I know of.
me013: Yeah, cuz everybody's scared.
mn052: Yeah.
me013: You'll see a little smoke coming up from the - the CPU or something trying to - trying to do it, but
me018: That's right
me013: uh, yeah. But, you're right that - that - that's a real good point, that uh, we - we don't know yeah, uh, I mean, what if any of these ta- I guess that's why they're pushing that in the uh - in the evaluation.
mn052: Yeah.
me013: Uh, But um, Good. OK. Anything else going on? at you guys' end, or - ?
fn002: I don't have good result, with the - inc- including the new parameters, I don't have good result. Are similar or a little bit worse.
mn052: With what - what other new p- new parameter?
me006: You're talking about your voicing?
me013: Yeah. So maybe - You probably need to back up a bit seeing as how Sunil, yeah .
fn002: Yeah. Mm-hmm.
mn052: Yeah.
fn002: I tried to include another new parameter to the traditional parameter, the coe- the cepstrum coefficient, that, like, the
mn052: Uh-huh.
fn002: auto-correlation, the Rzero
mn052: Mm-hmm.
fn002: and Rone over Rzero
mn052: Mm-hmm.
fn002: and another estimation of the var- the variance of the difference for - of the spec- si- uh, spectrum of the signal and - and the spectrum of time after filt- mel filter bank.
mn052: I'm so sorry. I didn't get it.
fn002: Nuh. Well. Anyway. The - First you have the sp- the spectrum of the signal, and you have the -
mn052: Mm-hmm.
fn002: on the other side you have the output of the mel filter bank.
mn052: Mm-hmm.
fn002: You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal.
mn052: Mmm. OK.
fn002: I do the difference - I found a difference at the variance of this different because, suppose
mn052: OK. Uh-huh.
fn002: we - we think that if the variance is high, maybe you have n- uh, noise.
mn052: Yeah.
fn002: And if the variance is small, maybe you have uh, speech.
mn052: Uh-huh.
fn002: To - to To - The idea is to found another feature for discriminate between voice sound and unvoice sound.
mn052: OK.
fn002: And we try to use this new feature - feature. And I did experiment - I need to change - to obtain this new feature I need to change the size - the window size - size. of the a- of the - analysis window size, to have more information.
mn052: Yeah. Make it longer.
fn002: Uh, sixty-two point five milliseconds I think.
mn052: OK.
fn002: And I do - I did two type of experiment to include this feature directly with the - with the other feature and to train a neural network to select it voice-unvoice-silence - silence and to - to concat this new feature. But the result are
mn052: Unvoiced. Well.
fn002: n- with the neural network I have more or less the same result.
mn052: As using just the cepstrum, or - ?
fn002: Result. Yeah. Yeah.
mn052: OK.
fn002: It's neve- e- e- sometime it's worse, sometime it's a little bit better, but not significantly. And -
mn052: Uh, is it with TI-digits, or with - ?
fn002: No, I work with eh, Italian and Spanish basically.
mn052: OK. OK.
fn002: And if I don't y- use the neural network, and use directly the feature
mn052: Uh-huh.
fn002: the results are worse. But Doesn't help.
me013: I - I - I really wonder though. I mean we've had these discussions before, and - and one of the things that struck me was that - uh, about this line of thought that was
mn007: Mm-hmm.
me013: particularly interesting to me was that we um - whenever you condense things, uh, in an irreversible way, um, you throw away some information. And, that's mostly viewed on as a good thing, in the way we use it, because we wanna suppress things that will cause variability for uh particular, uh, phonetic units. Um, but, you'll do throw something away. And so the question is, uh, can we figure out if there's something we've thrown away that we shouldn't have. And um. So, when they were looking at the difference between the filter bank and the FFT that was going into the filter bank, I was thinking "oh, OK, so they're picking on something they're looking on it to figure out noise, or voice - voiced property whatever." So that - that's interesting. Maybe that helps to drive the - the thought process of coming up with the features. But for me sort of the interesting thing was, "well, but is there just something in that difference which is useful? " So another way of doing it, maybe, would be just to take the FFT uh, power spectrum, and feed it into a neural network, and then use it, you know, in combination, or alone, or - or whatever
fn002: To know -
me018: Wi- with what targets?
mn052: Voiced, unvoiced is like -
me013: Uh, no.
mn052: Oh. Or anything.
me013: No the - just the same - same way we're using - I mean, the same way that we're using the filter bank.
me018: Phones.
mn052: Oh, OK.
mn007: Mm-hmm.
me013: Exact way - the same way we're using the filter bank. I mean, the filter bank is good for all the reasons that we say it's good. But it's different. And, you know, maybe if it's used in combination, it will get at something that we're missing.
mn007: Mm-hmm.
me013: And maybe, you know, using, orth- you know, KLT, or uh, um, adding probabilities, I mean, all th- all the different ways that we've been playing with, that we would let the - essentially let the neural network determine what is it that's useful, that we're missing here.
mn007: Mm-hmm. Yeah, but there is so much variability in the power spectrum.
mn052: Mm-hmm.
me013: Well, that's probably why y- i- it would be unlikely to work as well by itself, but it might help in combination.
mn007: Mm-hmm. Mmm.
me013: But I - I - I have to tell you, I can't remember the conference, but, uh, I think it's about ten years ago, I remember going to one of the speech conferences and - and uh, I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front-end or something, and a couple posters away it was somebody who compared one to uh, just putting in the FFT and the FFT did slightly better.
mn007: Mm-hmm.
me013: So I mean the - i- i- It's true there's lots of variability, but again we have these wonderful statistical mechanisms for quantifying that a- that variability, and you know, doing something reasonable with it. So, um,
mn007: Mm-hmm.
me013: uh, It- it's same, you know, argument that's gone both ways about uh, you know, we have these data driven filters, in LDA, and on the other hand, if it's data driven it means it's driven by things that have lots of variability, and that are necessarily - not necessarily gonna be the same in training and test, so, in some ways it's good to have data driven things, and in some ways it's bad to have data driven things. So, part of what we're discovering, is ways to combine things that are data driven than are not.
mn052: Yeah, d- Yeah.
me013: Uh, so anyway, it's just a thought, that - that if we - if we had that - maybe it's just a baseline uh, which would show us "well, what are we really getting out of the filters", or maybe i- i- probably not by itself, but in combination,
mn007: Mm-hmm.
me013: uh, you know, maybe there's something to be gained from it, and let the - But, you know, y- you've only worked with us for a short time, maybe in a year or two you w- you will actually come up with the right set of things to extract from this information. But, maybe the neural net and the HMMs could figure it out quicker than you. So. It's just a thought.
fn002: Maybe. Yeah, I can - I will try to do that.
me013: Yeah.
mn052: What - one - one um p- one thing is like what - before we started using this VAD in this Aurora, the - th- what we did was like, I - I guess most of you know about this, adding this additional speech-silence bit to the cepstrum and training the HMM on that.
me013: Mm-hmm.
mn052: That is just a binary feature and that seems to be improving a lot on the SpeechDat-Car where there is a lot of noise but not much on the TI-digits. So, a- adding an additional feature to distin- to discriminate between speech and nonspeech was helping.
mn007: Wait - I - I'm sorry?
mn052: That's it. Yeah, we actually added an additional binary feature to the cepstrum, just the baseline.
mn007: Yeah?
fn002: You did some experiment.
mn052: Yeah, yeah. Well, in - in the case of TI-digits it didn't actually give us anything, because there wasn't any f- anything to discriminate between speech, and it was very short.
mn007: Yeah. Hmm.
mn052: But Italian was like
mn007: Well -
mn052: very - it was a huge improvement on Italian.
mn007: Mm-hmm. But anyway the question is even more, is within speech, can we get some features? Are we drop- dropping information that can might be useful within speech, I mean. To - maybe to distinguish between voice sound and unvoiced sounds?
mn052: OK. Mm-hmm. Yeah, yeah. Yeah.
me013: And it's particularly more relevant now since we're gonna be given the endpoints.
mn007: Yeah. Mm-hmm.
me013: So.
mn052: Yeah, yeah.
me013: Uh.
mn007: Mmm.
me013: So. Um.
mn052: Mmm. There was a paper in ICASSP - this ICASSP - over the uh extracting some higher-order uh, information from the cepstral coefficients and I forgot the name. Some is- some harmonics I don't know, I can - I can pull that paper out from ICASSP. It - Huh?
mn007: Yeah.
me013: Talking cumulants or something? Cumulants or something. But - No.
mn052: Uh, I don't know. I don't remember. It wa- it was taking the, um - It was about finding the higher-order moments of - Yeah. And I'm not sure about whether it is the higher-order moments, or -
me013: Yeah, cumulants, yeah.
mn052: maybe higher-order cumulants and -
me013: Oh.
mn052: Yeah.
me013: Or m- e-
mn052: It was - it was - Yeah. I mean, he was showing up uh some - something on noisy speech, some improvement on the noisy speech.
me013: Yeah.
mn007: Mm-hmm.
mn052: Some small vocabulary tasks.
me013: Uh.
mn052: So it was on PLP derived cepstral coefficients.
me013: Yeah, but again - You could argue that th- that's exactly what the neural network does.
mn052: Mmm.
me013: So n- neural network uh, is in some sense equivalent to computing, you know, higher-order moments of what you - yeah.
mn052: trying to f- to Moments, yeah. Yeah.
me013: So. I mean, it doesn't do it very specifically, and
mn007: Mm-hmm.
me013: pretty - you know. But.
mn052: Yep.
me013: Uh, anything on your end you want to talk about? Uh.
me006: Um, nothing I wanna really talk about. I can - I can just uh, um, share a little bit - Sunil hasn't - hasn't heard about uh, what I've been doing. Um, so,
me013: Yeah.
me006: um, I told you I was - I was - I was getting prepared to take this qualifier exam. So basically that's just, um, trying to propose um, uh, your next your - your following years of - of your PHD work, trying - trying to find a project to - to define and - and to work on. So, I've been, uh, looking into, um, doing something about r- uh, speech recognition using acoustic events. So, um, the idea is you have all these - these different events, for example voicing, nasality, Rcoloring, you know burst or noise, uh, frication, that kinda stuff, um, building robust um, primary detectors for these acoustic events, and using the outputs of these robust detectors to do speech recognition. Um, and, um, these - these primary detectors, um, will be, uh, inspired by, you know, multi-band techniques, um, doing things, um, similar to Larry Saul's work on, uh, graphical models to - to detect these - these, uh, acoustic events. And, um, so I - I been - I been thinking about that and some of the issues that I've been running into are, um, exactly what - what kind of acoustic events I need, what - um, what acoustic events will provide a - a good enough coverage to - in order to do the later recognition steps. And, also, um, once I decide a set of acoustic events, um, h- how do I - how do I get labels? Training data for - for these acoustic events. And, then later on down the line, I can start playing with the - the models themselves, the - the primary detectors. Um, so, um, I kinda see - like, after - after building the primary detectors I see um, myself taking the outputs and feeding them in, sorta tandem style into - into a um, Gaussian mixtures HMM back-end, um, and doing recognition. Um. So, that's - that's just generally what I've been looking at. Um,
mn052: Yeah.
me013: By - by the way, uh, the voiced-unvoiced version of that for instance could tie right in to what Carmen was looking at. So,
me006: Yeah.
mn007: Mm-hmm.
me013: you know, um, if you - if a multi-band approach was helpful as - as I think it is, it seems to be helpful for determining voiced-unvoiced, that one might be another
me006: Mm-hmm. Yeah.
me013: thing.
fn002: Mm-hmm.
me006: Yeah. Um, were - were you gonna say something? Oh. It looked - OK, never mind. Um, yeah. And so, this - this past week um, I've been uh, looking a little bit into uh, TRAPS
me018: Mmm.
me006: um, and doing - doing TRAPS on - on these e- events too, just, um, seeing - seeing if that's possible. Uh, and um, other than that, uh, I was kicked out of Ihouse for living there for four years.
me013: Oh no. So you live in a cardboard box in the street now or, no?
me006: Yeah. Uh, well, s- s- som- something like that. In Albany, yeah. Yeah.
me013: Yeah.
me006: And uh. Yep. That's it.
me013: Suni- i- d'you v- did uh - did you find a place? Is that out of the way?
mn052: Uh, no not yet. Uh, yesterday I called up a lady who ha- who will have a vacant room from May thirtieth and she said she's interviewing two more people. So. And she would get back to me on Monday. So that's - that's only thing I have and Diane has a few more houses. She's going to take some pictures and send me after I go back.
me013: OK.
mn052: So it's - that's -
me018: Oh. So you're not down here permanently yet?
mn052: No. I'm going back to OGI today.
me018: Ah! Oh, OK.
me006: Oh.
me013: OK. And then, you're coming back uh -
mn052: Uh, i- I mean, I - I p- I plan to be here on thirty-first.
me013: Thirty-first, OK.
me006: Thirty-first.
mn052: Yeah, well if there's a house available or place to - Yeah, I hope.
me013: Well, I mean i- i- if - if - They're available, and they'll be able to get you something, so worst comes to worst we'll put you up in a hotel for - for - for a while until you -
mn052: Yeah. So, in that case, I'm going to be here on thirty-first definitely .
me013: OK.
me026: You know, if you're in a desperate situation and you need a place to stay, you could stay with me for a while. I've got a spare bedroom right now.
mn052: Oh. OK. Thanks. That sure is nice of you. So, it may be he needs more than me.
me006: Oh r- oh. Oh no, no. My - my cardboard box is actually a nice spacious two bedroom apartment.
me013: So a two bedroom cardboard box. Th- that's great. Thanks Dave.
me006: yeah
mn052: Yeah. Yeah. Yeah. Yeah.
me013: Um,
mn052: Yeah.
me013: Do y- wanna say anything about - You - you actually been - Uh, last week you were doing this stuff with Pierre, you were - you were mentioning. Is that - that something worth talking about, or - ?
me026: Um, it's - Well, um, it - I don't think it directly relates. Um, well, so, I was helping a speech researcher named Pierre Divenyi and he's int- He wanted to um, look at um, how people respond to formant changes, I think. Um. So he - he created a lot of synthetic audio files of vowel-to-vowel transitions, and then he wanted a psycho-acoustic um, spectrum. And he wanted to look at um, how the energy is moving over time in that spectrum and compare that to the - to the listener tests. And, um. So, I gave him a PLP spectrum. And - to um - he - he t- wanted to track the peaks so he could look at how they're moving. So I took the um, PLP LPC coefficients and um, I found the roots. This was something that Stephane suggested. I found the roots of the um, LPC polynomial to, um, track the peaks in the, um, PLP LPC spectra.
mn052: well there is aligned spectral pairs, is like the - the - Is that the aligned s-
me013: It's a r- root LPC, uh, of some sort.
mn052: Oh, no. So you just -
mn007: Mm-hmm.
me013: Yeah.
mn052: instead of the log you took the root square, I mean cubic root or something. What di- w- I didn't get that.
me013: No, no. It's - it's - it's taking the - finding the roots of the LPC polynomial.
mn052: Polynomial. Yeah. Is that the line spectral -
me013: So it's like line spectral pairs. Except I think what they call line spectral pairs they push it towards the unit circle, don't they, to sort of?
mn052: Oh, it's like line sp- Yeah, yeah, yeah, yeah.
me013: But it - But uh, you know. But what we'd used to do w- when I did synthesis at National Semiconductor twenty years ago, the technique we were playing with initially was - was taking the LPC polynomial and - and uh, finding the roots. It wasn't PLP cuz Hynek hadn't invented it yet, but it was just LPC, and uh, we found the roots of the polynomial, And th- When you do that, sometimes they're f- they're what most people call formants, sometimes they're not.
mn052: Mmm.
me013: So it's - it's - it's a little, uh - Formant tracking with it can be a little tricky cuz you get these funny values in - in real speech, but.
mn007: Hmm.
me018: So you just - You typically just get a few roots? You know, two or three, something like that?
me013: Well you get these complex pairs. And it depends on the order that you're doing, but.
mn007: Mm-hmm.
me026: Right. So, um, if - @@ Every root that's - Since it's a real signal, the LPC polynomial's gonna have real coefficients. So I think that means that every root that is not a real root
me018: Mm-hmm.
me026: is gonna be a c- complex pair, um, of a complex value and its conjugate. Um. So for each - And if you look at that on the unit circle, um, one of these - one of the members of the pair will be a positive frequency, one will be a negative frequency, I think. So I just - So, um, f- for the - I'm using an eighth-order polynomial and I'll get three or four of these pairs
me013: Yeah.
me026: which give me s- which gives me three or four peak positions.
mn052: Hmm.
me013: This is from synthetic speech, or - ?
me026: It's - Right. Yeah.
me013: Yeah. So if it's from synthetic speech then maybe it'll be cleaner. I mean for real speech in real - then what you end up having is, like I say, funny little things that are - don't exactly fit your notion of formants
me018: How did -
me013: all that well. But - but mostly they are. Mostly they do. And - and what - I mean in - in what we were doing,
mn007: But- Yeah.
me026: Mmm,
mn007: I -
me013: which was not so much looking at things, it was OK because it was just a question of quantization. Uh, we were just you know, storing - It was - We were doing, uh, stored speech, uh, quantization. But - but uh, in your case
mn007: Mm-hmm.
me013: um, you know -
mn007: Actually you have peaks that are not at the formant's positions, but
me026: But - there's some of that, yes.
mn007: they are lower in energy and - Well they are much lower.
me018: If this is synthetic speech can't you just get the formants directly? I mean h- how is the speech created?
me026: It was created from a synthesizer, and um -
me018: Wasn't a formant synthesizer was it?
me013: I bet it - it might have - may have been but maybe he didn't have control over it or something?
me026: I - d- d- this - In - in fact w- we - we could get, um, formant frequencies out of the synthesizer, as well. And, um, w- one thing that the, um, LPC approach will hopefully give me in addition, um, is that I - I might be able to find the b- the bandwidths of these humps as well. Um, Stephane suggested looking at each complex pair as a - like a se- second-order IIR filter. Um, but I don't think
me013: Yeah.
me026: there's a g- a really good reason not to um, get the formant frequencies from the synthesizer instead. Except that you don't have the psycho-acoustic modeling in that.
me013: Yeah, so the actual - So you're not getting the actual formants per se. You're getting the - Again, you're getting sort of the, uh -
mn007: Mm-hmm.
me013: You're getting something that is - is uh, af- strongly affected by the PLP model. And so it's more psycho-acoustic. So it's a little - It's - It's - It's sort of - sort of a different thing.
me018: Oh, I see. That's sort of the point.
me013: But - Yeah. i- Ordinarily, in a formant synthesizer, the bandwidths as well as the ban- uh, formant centers are - I mean, that's - Somewhere in the synthesizer that was put in, as -
me018: Yeah.
me026: Mm-hmm.
me013: as what you - But - but yeah, you view each complex pair as essentially a second-order section, which has, uh, band center and band width, and um, um - But. Yeah. O K. So, uh, yeah, you're going back today and then back in a week I guess, and.
mn052: Yeah.
me013: Yeah. Great! Well, welcome.
mn052: Thanks.
me018: I guess we should do digits quickly.
mn007: Mmm.
me013: Oh yeah, digits. I almost forgot that. I almost forgot our daily digits.
fn002: Digits.
me018: You wanna go ahead?
me013: Sure.
me018: OK.