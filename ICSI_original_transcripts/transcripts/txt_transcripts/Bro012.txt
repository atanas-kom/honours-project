me006: Hello?
me018: OK. We're on.
me013: OK, so uh had some interesting mail from uh Dan Ellis. Actually, I think he - he redirected it to everybody also so uh the PDA mikes uh have a big bunch of energy at - at uh five hertz uh where this came up was that uh I was showing off these wave forms that we have on the web and - and uh I just sort of hadn't noticed this, but that - the major, major component in the wave - in the second wave form in that pair of wave forms is actually the air conditioner.
me026: Huh.
me013: So. So. I I have to be more careful about using that as a - as a - as a good illustration, uh, in fact it's not, of uh - of the effects of room reverberation. It is- isn't a bad illustration of the effects of uh room noise. on - on uh some mikes uh but So. And then we had this other discussion about um whether this affects the dynamic range, cuz I know, although we start off with thirty two bits, you end up with uh sixteen bits and you know, are we getting hurt there? But uh Dan is pretty confident that we're not, that - that quantization error is not - is still not a significant factor there. So. So there was a question of whether we should change things here, whether we should change a capacitor on the input box for that or whether we should
me018: Yeah, he suggested a smaller capacitor, right? For the PDAs?
me013: Right. But then I had some other uh thing- discussions with him and the feeling was once we start monk- monkeying with that, uh, many other problems could ha- happen. And additionally we - we already have a lot of data that's been collected with that, so.
me018: Yeah.
me013: A simple thing to do is he - he - he has a - I forget if it - this was in that mail or in the following mail, but he has a - a simple filter, a digital filter that he suggested. We just run over the data before we deal with it.
me018: Mm-hmm.
me013: um The other thing that I don't know the answer to, but when people are using Feacalc here, uh whether they're using it with the high-pass filter option or not. And I don't know if anybody knows. But.
me006: Um. I could go check.
me013: Yeah. So when we're doing all these things using our software there is - um if it's - if it's based on the RASTA-PLP program, which does both PLP and RASTA-PLP um then uh there is an option there which then comes up through to Feacalc which um allows you to do high-pass filtering and in general we like to do that, because of things like this and it's - it's pretty - it's not a very severe filter. Doesn't affect speech frequencies, even pretty low speech frequencies, at all, but it's
me018: What's the cut-off frequency it used?
me013: Oh. I don't know I wrote this a while ago
me018: Is it like twenty?
me013: Something like that. Yeah. I mean I think there's some effect above twenty but it's - it's - it's - it's mild.
me018: Yeah.
me013: So, I mean it probably - there's probably some effect up to a hundred hertz or something but it's - it's pretty mild. I don't know in the - in the STRUT implementation of the stuff is there a high-pass filter or a pre- pre-emphasis or something in the -
mn007: Uh. I think we use a pre-emphasis. Yeah. Yeah.
me013: So. We - we - we want to go and check that in i- for anything that we're going to use the PD A mike for. uh He says that there's a pretty good roll off in the PZM mikes so we don't need - need to worry about them one way or the other but if we do make use of the cheap mikes, uh we want to be sure to do that - that filtering before we process it. And then again if it's uh depending on the option that the - our - our software is being run with, it's - it's quite possible that's already being taken care of. uh But I also have to pick a different picture to show the effects of reverberation. uh
me018: Did somebody notice it during your talk?
me013: uh No.
me018: Huh.
me013: Well. uh Well. If they made output they were - they were, you know - they were nice.
me018: Didn't say anything?
me013: But. I mean the thing is it was since I was talking about reverberation and showing this thing that was noise, it wasn't a good match, but it certainly was still uh an indication of the fact that you get noise with distant mikes.
me018: Mm-hmm.
me013: uh It's just not a great example because not only isn't it reverberation but it's a noise that we definitely know what to do. So, I mean, it doesn't take deep - a new - bold new methods to get rid of uh five hertz noise, so.
me018: Yeah.
me013: um uh But. So it was - it was a bad example in that way, but it's - it still is - it's the real thing that we did get out of the microphone at distance, so it wasn't it w- it w- wasn't wrong it was inappropriate. So. So uh, but uh, Yeah, someone noticed it later pointed it out to me, and I went "oh, man. Why didn't I notice that?"
me018: Hmm.
me013: um. So. um So I think we'll change our - our picture on the web, when we're @@ . One of the things I was - I mean, I was trying to think about what - what's the best way to show the difference an- and I had a couple of thoughts one was, that spectrogram that we show is O K, but the thing is the eyes uh and the the brain behind them are so good at picking out patterns from - from noise that in first glance you look at them it doesn't seem like it's that bad uh because there's many features that are still preserved. So one thing to do might be to just take a piece of the spec- uh of the spectrogram where you can see that something looks different, an- and blow it up, and have that be the part that's - just to show as well. You know.
me018: Mm-hmm. Mm-hmm.
me013: i- i- Some things are going to be hurt. um Another, I was thinking of was um taking some spectral slices, like uh - like we look at with the recognizer, and look at the spectrum or cepstrum that you get out of there, and the - the uh, um, the reverberation uh does make it - does change that. And so maybe - maybe that would be more obvious.
me018: Hmm.
me026: Spectral slices?
me013: Yeah.
me026: W- w- what d- what do you mean?
me013: Well, I mean um all the recognizers look at frames. So they - they look at -
me018: So like one instant in time.
me026: OK.
me013: Yeah, look at a - So it's, yeah, at one point in time or uh twenty - over twenty milliseconds or something,
me026: OK.
me013: you have a spectrum or a cepstrum. That's what I meant by a slice. Yeah. And
me026: I see.
me018: You could just - you could just throw up, you know, uh
me013: if you look at -
me018: the uh - some MFCC feature vectors. You know, one from one, one from the other, and then, you know, you can look and see how different the numbers are.
me013: Right. Well, that's why I saying either Well, either spectrum or cepstrum but - but I think the thing is you wanna -
me018: I'm just kidding. I don't mean a graph. I mean the actual numbers.
me013: Oh. I see. Oh. That would be lovely, yeah.
me018: Yeah. "See how different these sequences of numbers are?"
me013: Yeah. Or I could just add them up and get a different total.
me018: Yeah. It's not the square.
me013: OK. Uh. What else - wh- what's - what else is going on?
mn007: Uh, yeah. Yeah, at first I had a remark why - I am wondering why the PDA is always so far. I mean we are always meeting at the beginning of the table and the PDA's there.
me013: Uh. I guess cuz we haven't wanted to move it. We - we could - we could move us, and.
mn007: Yeah?
me006: That's right.
mn007: OK. Well, anyway. Um. Yeah, so. Uh. Since the last meeting we've - we've tried to put together um the clean low-pass um downsampling, upsampling, I mean, Uh the new filter that's replacing the LDA filters, and also the um delay issue so that - We considered th- the - the delay issue on the - for the on-line normalization. Mmm. So we've put together all this and then we have results that are not um very impressive. Well, there is no real improvement.
me013: But it's not wer- worse and it's better - better latency, right?
mn007: It's not - Yeah. Yeah. Well. Actually it's better. It seems better when we look at the mismatched case but I think we are like - like cheated here by the - th- this problem that uh in some cases when you modify slight - slightly modify the initial condition you end up completely somewhere air- somewhere else in the - in the space, the parameters. So.
me013: Yeah.
mn007: Well. The other system are for instance. For Italian is at seventy-eight percent recognition rate on the mismatch, and this new system has eighty-nine. But I don't think it indicates something, really. I don't - I don't think it means that the new system is more robust or -
me013: Uh-huh.
mn007: It's simply the fact that - Well.
me013: Well, the test would be if you then tried it on one of the other test sets, if -
mn007: Y-
me013: if it was - Right. So this was Italian, right?
mn007: Yeah. Yeah. It's similar for other test sets but I mean
me013: So then if you take your changes and then -
mn007: from this se- seventy-eight um percent recognition rate system,
me013: Uh-huh.
mn007: I could change the transition probabilities for the - the first HMM and it will end up to eighty-nine also. By using point five instead of point six, point four as in the - the HTK script.
me013: Uh-huh. Yeah.
mn007: So. Well. That's -
me018: Yeah. Yeah I looked at um - looked at the results when Stephane did that and it's - it's really wo- really happens. I mean th- the only difference is you change the self-loop transition probability by a tenth of a percent and it causes ten percent difference in the word error rate.
mn007: Well. Eh uh - This really happens. Yeah.
me013: Yeah. A tenth of a per cent.
mn007: Even tenth of a percent?
me018: Yeah. From point -
mn007: Well, we tried - we tried point one, yeah.
me018: I - I'm sorry f- for point - from - You change at point one and n- not tenth of a percent, one tenth, alright ? Um so from point five - so from point six to point five and you get ten percent better.
mn007: Hmm.
me013: Oh! Yeah. Mm-hmm.
mn007: Mm-hmm.
me018: And it's - I think it's what you basically hypothesized in the last meeting about uh it just being very - and I think you mentioned this in your email too - it's just very um - you know get stuck in some local minimum and this thing throws you out of it I guess.
mn007: Mmm, yeah. Mm-hmm.
me013: Well, what's - what are - according to the rules what - what are we supposed to do about the transition probabilities? Are they supposed to be point five or point six?
me018: I think you're not allowed to - Yeah. That's supposed to be point six,
mn007: Yeah.
me018: for the self-loop.
me013: Point - It's supposed to be point six.
me018: Yeah. But changing it to point five I think is - which gives you much better results, but that's not allowed.
me013: But not allowed? Yeah. OK .
me018: Yeah.
mn007: Yeah, but even if you use point five, I'm not sure it will always give you the better results on other test set or it
me018: Yeah. Right. We only tested it on the - the medium mismatch, right? You said on the other cases you didn't notice -
mn007: on the other training set, I mean. Yeah. But. I think, yeah. I think the reason is, yeah, I not- I - it was in my mail I think also, is the fact that the mismatch is trained only on the far microphone. Well, in - for the mismatched case everything is um using the far microphone training and testing, whereas for the highly mismatched, training is done on the close microphone so it's - it's clean speech basically so you don't have this problem of local minima probably and for the well-match, it's a mix of close microphone and distant microphone and - Well.
me018: I did notice uh something -
mn007: So th- I think the mismatch is the more difficult for the training part.
me018: Somebody, I think it was Morgan, suggested at the last meeting that I actually count to see how many parameters and how many frames.
mn007: Mm-hmm.
me013: Mm-hmm.
me018: And there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system.
me013: Hmm.
mn007: Yeah.
me018: So it's very, very few parameters compared to how much training data.
fn002: Mm-hmm.
me013: Well. Yes. So. And that - that says that we could have lots more parameters actually.
me018: Yeah. Yeah. I did one quick experiment just to make sure I had everything worked out and I just -
mn007: Mm-hmm.
me018: uh f- for most of the um - For - for all of the digit models, they end up at three mixtures per state. And so I just did a quick experiment, where I changed it so it went to four and um it it - it didn't have a r- any significant effect at the uh medium mismatch and high mismatch cases and it had - it was just barely significant for the well-matched better. Uh so I'm r- gonna run that again but um with many more uh mixtures per state.
me013: Yeah. Cuz at forty thou- I mean you could you could have uh - Yeah, easily four times as many parameters.
me018: Mm-hmm. And I think also just seeing what we saw uh in terms of the expected duration of the silence model? when we did this tweaking of the self-loop?
mn007: Yeah.
me018: The silence model expected duration was really different. And so in the case where um it had a better score, the silence model expected duration was much longer. So it was like - it was a better match. I think you know if we
mn007: Yeah.
me018: make a better silence model I think that will help a lot too um for a lot of these cases so but one one thing I - I wanted to check out before I increased the um number of mixtures per state was uh in their default training script they do an initial set of three re-estimations and then they built the silence model and then they do seven iterations then the add mixtures and they do another seven then they add mixtures then they do a final set of seven and they quit. Seven seems like a lot to me and it also makes the experiments go take a really long time I mean to do one turn-around of the well matched case takes like a day.
me013: Mm-hmm. Mm-hmm.
me018: And so you know in trying to run these experiments I notice, you know, it's difficult to find machines, you know, compute the run on. And so one of the things I did was I compiled HTK for the Linux
me013: Mm-hmm.
me018: machines cuz we have this one from IBM that's got like five processors in it?
me013: Right.
me018: and so now I'm - you can run stuff on that and that really helps a lot because now we've got you know, extra machines that we can use for compute. And if - I'm do- running an experiment right now where I'm changing the number of iterations? from seven to three? just to see how it affects the baseline system.
fn002: Mm-hmm.
me013: Yeah.
me018: And so if we can get away with just doing three, we can do many more experiments more quickly. And if it's not a - a huge difference from running with seven iterations,
mn007: Hmm.
me018: um, you know, we should be able to get a lot more experiments done. And so. I'll let you know what - what happens with that. But if we can you know, run all of these back-ends f- with many fewer iterations and on Linux boxes we should be able to get a lot more experimenting done.
me013: Mm-hmm.
me018: So. So I wanted to experiment with cutting down the number of iterations before I increased the number of Gaussians.
me013: Right. Sorry.
mn007: Um.
me013: So um, how's it going on the - So. You - you did some things. They didn't improve things in a way that convinced you you'd substantially improved anything.
mn007: Yeah.
me013: But they're not making things worse and we have reduced latency, right?
mn007: Yeah. But actually - um actually it seems to do a little bit worse for the well-matched case and we just noticed that - Yeah, actually the way the final score is computed is quite funny. It's not a mean of word error rate. It's not a weighted mean of word error rate, it's a weighted mean of improvements.
me013: Uh-huh.
mn007: So. Which means that actually the weight on the well-matched is - Well I well what what - What happened is that if you have a small improvement or a small if on the well-matched case it will have uh huge influence on the improvement compared to the reference because the reference system is - is - is quite good for - for the well-ma- well-matched case also.
me018: So it - it weights the improvement on the well-matched case really heavily compared to the improvement on the other cases?
mn007: No, but it's the weighting of the - of the improvement not of the error rate.
me018: Yeah. Yeah, and it's hard to improve on the - on the best case, cuz it's already so good, right?
mn007: Yeah but what I mean is that you can have a huge improvement on the H - HMK's, uh like five percent uh absolute, and this will not affect the final score almost - Uh this will almost not affect the final score because this improvement - because the improvement uh relative to the - the baseline is small -
me013: So they do improvement in terms of uh accuracy? rather than word error rate?
mn007: Uh. Uh improvement? No, it's compared to the word er- it's improvement on the word error rate, yeah. Sorry.
me013: So - OK. So if you have uh ten percent error and you get five percent absolute uh improvement then that's fifty percent.
mn007: Mm-hmm.
me013: OK. So what you're saying then is that if it's something that has a small word error rate,
mn007: Mm-hmm.
me013: then uh a - even a relatively small improvement on it, in absolute terms, will show up as quite - quite large in this. Is that what you're saying? Yes.
mn007: Yeah. Yeah.
me013: OK. But yeah that's - that's - it's the notion of relative improvement.
mn007: Yeah.
me013: Word error rate.
mn007: Sure, but when we think about the weighting, which is point five, point three, point two, it's on absolute on - on relative figures, not -
me013: Yeah. Yeah.
mn007: So when we look at this error rate
me013: No. That's why I've been saying we should be looking at word error rate uh and - and not - not at at accuracies. It's -
mn007: uh - Mmm, yeah. Mmm, yeah. Mm-hmm.
me013: I mean uh we probably should have standardized on that all the way through. It's just -
me018: Well.
mn007: Mm-hmm.
me018: I mean, it's not - it's not that different, right? I mean, just subtract the accuracy. I mean -
me013: Yeah but you're - but when you look at the numbers, your sense of the relative size of things is quite different. If you had ninety percent uh correct and five percent, five over ninety doesn't look like it's a big difference, but five over ten is - is big.
me018: Oh. Oh, I see. Yeah. Mm-hmm.
mn007: Mm-hmm.
me013: So just when we were looking at a lot of numbers and getting sense of what was important.
me018: I see. I see. Yeah. That makes sense.
me013: Um. Um.
mn007: Mmm. Well anyway uh. So. Yeah. So it hurts a little bit on the well-match and yeah.
me013: What's a little bit? Like -
mn007: Like, it's difficult to say because again um I'm not sure I have the um -
me018: Hey Morgan? Do you remember that Signif program that we used to use for testing signi- ? Is that still valid? I - I've been using that.
me013: Yeah. Yeah, it was actually updated. Uh.
me018: OK. Oh, it was. Oh, I shoul-
me013: Jeff updated it some years ago and - and uh cleaned it up made some things better in it. So.
me018: OK. I should find that new one. I just use my old one from ninety-two or whatever
me013: Yeah, I'm sure it's not that different but - but he - he uh - he was a little more rigorous, as I recall.
me018: OK.
mn007: Right. So it's around, like, point five. No, point six uh percent absolute on Italian -
me013: Worse.
mn007: Worse, yep.
me013: Out of what? I mean. s-
mn007: Uh well we start from ninety-four point sixty-four, and we go to ninety-four point O four.
me013: Uh-huh. So that's six - six point th-
mn007: Uh.
me018: Ninety- three point six four, right? is the baseline.
mn007: Oh, no, I've ninety-four. Oh, the baseline, you mean.
me018: Yeah.
mn007: Well I don't - I'm not talking about the baseline here. I uh -
me018: Oh. Oh. I'm sorry.
mn007: My baseline is the submitted system.
me018: Ah! OK. Ah, ah.
mn007: Hmm.
me013: Yeah.
me018: Sorry.
mn007: Oh yeah. For Finnish, we start to ninety-three point eight-four and we go to ninety-three point seventy-four. And for Spanish we are - we were at ninety-five point O five and we go to ninety-three-s- point sixty one.
me013: OK, so we are getting hurt somewhat. And is that wh- what - do you know what piece - you've done several changes here. Uh, do you know what pie-
mn007: So. Yeah. I guess - I guess it's - it's the filter. Because nnn, well uh we don't have complete result, but the filter - So the filter with the shorter delay hurts on Italian well-matched, which - And, yeah. And the other things, like um downsampling, upsampling, don't seem to hurt and
me018: I'm -
mn007: the new on-line normalization, neither. So.
me018: I'm really confused about something. If we saw that making a small change like, you know, a tenth, to the self-loop had a huge effect, can we really make any conclusions about
mn007: Mm-hmm. Yeah that's th-
me018: differences in this stuff? I mean, especially when they're this small. I mean.
mn007: Yeah. I think we can be completely fooled by this thing, but - I don't know.
me013: Well, yeah.
mn007: So. There is first this thing, and then the - yeah, I computed the um - like, the confidence level on the different test sets. And for the well-matched they are around um point six uh percent. For the mismatched they are around like let's say one point five percent. And for the well-m- uh HM they are also around one point five.
me013: But - OK, so you - these - these degradations you were talking about were on the well-matched case
mn007: So. Yeah.
me013: Uh. Do the - does the new filter make things uh better or worse for the other cases?
mn007: But. Uh. About the same. It doesn't hurt. Yeah.
me013: Doesn't hurt, but doesn't get a little better, or something. No.
mn007: No.
me013: OK, so um I guess the argument one might make is that, "Yeah, if you looked at one of these cases and you jiggle something and it changes then uh you're not quite sure what to make of it. But when you look across a bunch of these and there's some - some pattern, um - I mean, so eh h- here's all the - if - if in all these different cases it never gets better, and there's significant number of cases where it gets worse, then you're probably hurting things, I would say. So um I mean at the very least that would be a reasonably prediction of what would happen with - with a different test set, that you're not jiggling things with. So I guess the question is if you can do better than this. If you can - if we can approximate the old numbers while still keeping the latency down.
mn007: Mmm. Yeah.
me013: Uh, so. Um. What I was asking, though, is uh - are - what's - what's the level of communication with uh the OG I gang now, about this and -
mn007: Well, we are exchanging mail as soon as we -
me013: Yeah.
mn007: we have significant results. Um. Yeah. For the moment, they are working on integrating the um spectral subtraction
me013: Mm-hmm.
mn007: apparently from Ericsson. Um. Yeah. And so. Yeah. We are working on our side on other things like uh also trying a sup- spectral subtraction but of - of our own, I mean, another
me013: Mm-hmm.
mn007: spectral subtraction. Um. Yeah. So I think it's - it's OK. It's going -
me013: Is there any further discussion about this - this idea of - of having some sort of source code control?
mn007: Yeah. Well. For the moment they're - uh everybody's quite um - There is this Eurospeech deadline, so.
me013: I see.
mn007: Um. And. Yeah. But yeah. As soon as we have something that's significant and that's better than - than what was submitted, we will fix - fix the system and - But we've not discussed it - it - it - this yet, yeah.
me013: Yeah. Sounds like a great idea but - but I think that - that um he's saying people are sort of scrambling for a Eurospeech deadline. But that'll be uh, uh done in a week. So, maybe after this next one.
mn007: Mmm. Yeah.
me018: Wow! Already a week! Man! You're right. That's amazing.
me013: Yeah. Yeah. Anybo- anybody in the - in this group do- doing anything for Eurospeech? Or, is that what - is that -
mn007: S- Yeah we are - We are trying to - to do something with the Meeting Recorder digits,
me013: Right.
mn007: and - But yeah. Yeah. And the good thing is that there is this first deadline,
me013: Yeah.
mn007: and, well, some people from OGI are working on a paper for this, but there is also the um special session about th- Aurora which is - uh which has an extended deadline. So. The deadline is in May.
me013: For uh - Oh, for Eurospeech?
mn007: For th- Yeah. So f- only for the experiments on Aurora. So it - it's good, yeah.
me013: Oh! Oh, a special dispensation. That's great.
me018: Mm-hmm. Where is Eurospeech this year?
me013: Aalborg - Aalborg uh
mn007: It's in Denmark.
me018: Oh.
me013: So the deadline - When's the deadline?
mn007: Hmm?
me013: When's the deadline?
mn007: I think it's the thirteenth of May.
me013: That's great! It's great. So we should definitely get something in for that.
mn007: Yeah.
me013: But on meeting digits, maybe there's - Maybe. Maybe.
mn007: Yeah. So it would be for the first deadline.
me013: Yeah.
mn007: Nnn.
me013: Yeah. So, I mean, I - I think that you could certainly start looking at - at the issue uh but - but uh I think it's probably, on s- from what Stephane is saying, it's - it's unlikely to get sort of active participation from the two sides until after they've -
me018: Well I could at least - Well, I'm going to be out next week but I could try to look into like this uh CVS over the web. That seems to be a very popular way of people distributing changes and - over, you know, multiple sites and things so maybe
me013: Mm-hmm.
me018: if I can figure out how do that easily and then pass the information on to everybody so that it's you know, as easy to do as possible and - and people don't - it won't interfere with their regular work, then maybe that would be good. And I think we could use it for other things around here too. So.
me013: Good.
me026: That's cool. And if you're interested in using CVS, I've set it up here, so.
me018: Oh great. OK. I used it a long time ago but it's been a while so maybe I can ask you some questions.
me026: um j- Oh. So. I'll be away tomorrow and Monday but I'll be back on Tuesday or Wednesday.
me018: OK.
me013: Yeah. Dave, the other thing, actually, is - is this business about this wave form. Maybe you and I can talk a little bit at some point about coming up with a better uh demonstration of the effects of reverberation for our web page, cuz uh - the uh um I mean, actually the - the uh It made a good - good audio demonstration because when we could play that clip the - the - the really obvious difference is that you can hear two voices and - in the second one and only hear -
me018: Maybe we could just like, talk into a cup.
me013: Yeah.
me018: Some good reverb.
me013: No, I mean, it sound - it sounds pretty reverberant, but I mean you can't - when you play it back in a room with a - you know a big room, nobody can hear that difference really. They hear that it's lower amplitude and they hear there's a second voice, um but uh that - actually that makes for a perfectly good demo because that's a real obvious thing, that you hear two voices. Yeah.
me026: Yeah. Uh-huh.
me018: But not of reverberation.
me026: A boom.
me013: Well that - that - that's OK. But for the - the visual, just, you know, I'd like to have uh uh, you know, the spectrogram again, because you're - you're - you're visual uh abilities as a human being are so good you can pick out - you know, you - you look at the good one, you look at the cru- the screwed up one, and - and you can see the features in it without trying to @@ - yeah.
me026: Yeah.
me018: I noticed that in the pictures. I thought "hey, you know th-" I - My initial thought was "this is not too bad!"
me013: Right. But you have to - you know, if you look at it closely, you see "well, here's a place where this one has a big formant - uh uh formant - maj- major formants here are - are moving quite a bit." And then you look in the other one and they look practically flat.
me018: Mm-hmm.
me013: So I mean you could - that's why I was thinking, in a section like that, you could take a look - look at just that part of the spectrogram and you could say " Oh yeah. This - this really distorted it quite a bit."
me018: Yeah. The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies. It looked like for the one that was farther away, you know, it really - everything was attenuated and -
me013: Right.
me018: I mean that was the main visual thing that I noticed.
me013: Right. But it's - it's uh - So. Yeah. So there are - clearly are spectral effects. Since you're getting all this indirect energy, then a lot of it does have - have uh reduced high frequencies. But um the other thing is the temporal courses of things really are changed, and - and uh we want to show that, in some obvious way. The reason I put the wave forms in there was because uh they - they do look quite different. Uh. And so I thought "Oh, this is good." but I - I just uh - After - after uh they were put in there I didn't really look at them anymore, cuz I just - they were different. So I want something that has a - is a more interesting explanation for why they're different. Um.
me026: Oh. So maybe we can just substitute one of these wave forms and um then do some kind of zoom in on the spectrogram on an interesting area.
me013: Something like that. Yeah.
me026: Uh-huh.
me013: The other thing that we had in there that I didn't like was that um the most obvious characteristic of the difference uh when you listen to it is that there's a second voice, and the - the - the - the - the uh cuts that we have there actually don't correspond to the full wave form. It's just the first - I think there was something where he was having some trouble getting so much in, or. I - I forget the reason behind it. But it - it's um - it's the first six seconds or something of it and it's in the seventh or eighth second or something where @@ the second voice comes in. So we - we would like to actually see the voice coming in, too, I think, since that's the most obvious thing when you listen to it.
me026: Mm-hmm.
me013: So. Um.
mn007: Uh, yeah. Yeah. I brought some - I don't know if - some
me013: @@
mn007: figures here. Well. I start - we started to work on spectral subtraction. And um the preliminary results were very bad. So the thing that we did is just to add spectral subtraction before
me013: Uh-huh.
mn007: this, the Wall uh process, which contains LDA on-line normalization. And it hurts uh a lot.
me013: Uh-huh.
mn007: And so we started to look at - at um things like this, which is, well, it's - Yeah. So you have the Czero parameters for
fn002: You can @@ .
mn007: one uh Italian utterance. And I plotted this for two channels. Channel zero is the close mic- microphone, and channel one is the distant microphone. And it's perfectly synchronized, so. And the sentence contain only one word, which is "Due" And it can't clearly be seen. Where - where is it? Where is the word?
me013: Uh-huh.
me018: This is - this is, oh, a plot of Czero, the energy.
me006: Hmm.
mn007: So. This is a plot of Czero, uh when we don't use spectral subtraction, and when there is no on-line normalization. So.
me013: Mm-hmm.
mn007: There is just some filtering with the LDA and and some downsampling, upsampling.
me018: Czero is the close talking? - uh the close channel? and s- channel one is the -
mn007: So. Yeah. Yeah. Yeah. So Czero is very clean, actually.
me018: Yeah.
mn007: Uh then when we apply mean normalization it looks like the second figure, though it is not. Which is good. Well, the noise part is around zero and -
me013: Mm-hmm.
mn007: And then the third figure is what happens when we apply mean normalization and variance normalization. So. What we can clearly see is that on the speech portion the two channel come - becomes very close, but also what happens on the noisy portion is that the variance of the noise is -
me013: Mm-hmm.
me018: This is still being a plot of Czero?
mn007: Yeah. This is still Czero.
me018: OK. Can I ask um what does variance normalization do? w- What is the effect of that?
me013: Normalizes the variance.
mn007: So it - it -
me018: I mean
mn007: Yeah. It normalized th- the standard deviation. So it -
me018: y- Yeah. No, I understand that, but I mean -
mn007: You - you get an estimate of the standard deviation. That's um -
me018: No. No, I understand what it is, but I mean, what does it - what's - what is uh -
mn007: Yeah but.
me013: What's the rationale?
me018: We- Yeah. Yeah. Why - why do it?
mn007: Uh.
me013: Well, I mean, because everything uh - If you have a system based on Gaussians, everything is based on means and variances. So if there's an overall
me018: Yeah.
me013: reason - You know, it's like uh if you were doing uh image processing and in some of the pictures you were looking at, uh there was a lot of light uh and - and in some, there was low light, you know, you would want to adjust for that in order to compare things.
me018: Mm-hmm. Mm-hmm.
me013: And the variance is just sort of like the next moment, you know? So uh what if um one set of pictures was taken uh so that throughout the course it was - went through daylight and night uh um um ten times, another time it went thr- I mean i- is, you know, how - how much -
me018: Oh, OK.
me013: how much vari- Or no. I guess a better example would be how much of the light was coming in from outside rather than artificial light. So if it was a lot - if more was coming from outside, then there'd be the bigger effect of the - of the - of the change in the - So every mean - every - all - all of the - the parameters that you have, especially the variances, are going to be affected by the overall variance.
me018: Oh, OK. Uh-huh. I see. OK.
me013: And so, in principle, you - if you remove that source, then, you know, you can -
me018: So would - the major effect is - that you're gonna get is by normalizing the means, but it may help -
me013: That's the first order but - thing, but then the second order is - is the variances
me018: First-order effects. And it may help to do the variance. OK. OK.
me013: because, again, if you - if you're trying to distinguish between E and B
me018: Mm-hmm.
me013: if it just so happens that the E's were a more - you know, were recorded when - when the energy was - was - was larger or something, or the variation in it was larger,
me018: Mm-hmm. Mm-hmm. Mm-hmm.
me013: uh than with the B's, then this will be - give you some - some bias. So the -
me018: OK.
me013: it's removing these sources of variability in the data that have nothing to do with the linguistic component.
mn007: Mmm.
me018: Gotcha. OK . Sorry to interrupt.
me013: But the - the uh - but let me as- ask - ask you something. i- is - if -
mn007: Yep. And it - and this -
me013: If you have a good voice activity detector, isn't - isn't it gonna pull that out?
mn007: Yeah. Sure. If they are good. Yeah. Well what it - it shows is that, yeah, perhaps a good voice activity detector is - is good before on-line normalization and that's what uh we've already observed. But uh, yeah, voice activity detection is not an easy thing neither.
me018: But after you do this, after you do the variance normalization - I mean.
mn007: Mm-hmm.
me018: I don't know, it seems like this would be a lot easier than this signal to work with.
mn007: Yeah. So. What I notice is that, while I prefer to look at the second figure than at the third one, well, because you clearly see where speech is.
me013: Yeah.
me018: Yeah.
mn007: But the problem is that on the speech portion, channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer.
me013: Right.
me018: But for the purposes of finding the speech -
mn007: And - Yeah, but here - Yeah.
me018: You're more interested in the difference between the speech and the nonspeech, right?
mn007: Yeah. So I think, yeah. For I th- I think that it - perhaps it shows that uh the parameters that the voice activity detector should use - uh have to use should be different than the parameter that have to be used for speech recognition.
me013: Yeah. So basically you want to reduce this effect. So you can do that by doing the voi- voice activity detection. You also could do it by spect- uh spectral subtraction before the variance normalization, right?
mn007: Well, y- Yeah, but it's not clear, yeah.
me013: So uh -
mn007: We- So. Well. It's just to the - the number that at- that are here are recognition experiments on Italian HM and MM
me013: Yeah.
mn007: with these two kinds of parameters. And, well, it's better with variance normalization.
me013: Yeah. Yeah. So it does get better even though it looks ugly. OK.
mn007: Uh -
me013: but does this have the voice activity detection in it?
mn007: Yeah.
me013: OK.
mn007: Um.
me006: OK.
me013: So.
me018: Where's th-
mn007: But the fact is that the voice activity detector doesn't work on channel one. So. Yeah.
me013: Uh-huh.
me018: Where - at what stage is the voice activity detector applied? Is it applied here or a- after the variance normalization? or -
mn007: Hmm?
me013: Spectral subtraction, I guess.
mn007: It's applied before variance normalization. So it's a good thing, because
me018: Oh. Yeah.
mn007: I guess voice activity detection on this should - could be worse.
me018: Is it applied all the way back here?
mn007: It's applied the um on, yeah, something like this, yeah.
me018: Maybe that's why it doesn't work for channel one.
mn007: Perhaps, yeah.
me013: Can I -
mn007: So we could perhaps do just mean normalization before VAD.
me018: Mm-hmm.
me013: Mm-hmm. Can I ask a, I mean - a sort of top-level question, which is um "if - if most of what the OGI folk are working with is trying to integrate this other - other uh spectral subtraction, why are we worrying about it?"
mn007: Mm-hmm. About? Spectral subtraction?
me013: Yeah.
mn007: It's just uh - Well it's another - They are trying to u- to use the um - the Ericsson and we're trying to use something - something else. And. Yeah, and also to understand what happens because
me013: OK.
mn007: uh fff Well. When we do spectral subtraction, actually, I think that this is the - the two last figures.
me013: Yeah.
mn007: Um. It seems that after spectral subtraction, speech is more emerging now uh
me013: Mm-hmm.
me018: Speech is more what?
mn007: than - than before. Well, the difference between the energy of the speech and the energy of the n- spectral subtrac- subtracted noise portion is - is larger.
me013: Mm-hmm.
mn007: Well, if you compare the first figure to this one - Actually the scale is not the same, but if you look at the - the numbers um you clearly see that the difference between the Czero of the speech and Czero of the noise portion is larger. Uh but what happens is that after spectral subtraction, you also increase the variance of this - of Czero. And so if you apply variance normalization on this, it completely sc- screw
me013: Mm-hmm.
mn007: everything. Well.
me013: Mm-hmm.
mn007: Um. Uh. Yeah. So yeah. And what they did at OGI is just uh they don't use on-line normalization, for the moment, on spectral subtraction and I think - Yeah. I think as soon as they will try on-line normalization there will be a problem. So yeah, we're working on the same thing but I think uh with different - different system and -
me013: Right. I mean, i- the Intellectually it's interesting to work on things th- uh one way or the other but I'm - I'm just wondering if um - on the list of things that there are to do, if there are things that we won't do because we've got two groups doing the same thing. Um.
mn007: Mm-hmm. Mm-hmm. Mm-hmm.
me013: That's - Um. Just - just asking. Uh. I mean, it's -
mn007: Yeah, well, uh.
me018: There also could be - I mean. I can maybe see a reason f- for both working on it too if um you know, if - if - if you work on something else and - and you're waiting for them to give you spectral subtraction - I mean it's hard to know whether the effects that you get from the other experiments you do will carry over once you then bring in their spectral subtraction module. So it's - it's almost like everything's held up waiting for this one thing. I don't know if that's true or not, but I could see how -
mn007: Mmm.
me018: Maybe that's what you were thinking.
me013: I don't know. I don't know. I mean, we still evidently have a latency reduction plan which - which isn't quite what you'd like it to be. That - that seems like one prominent thing. And then uh weren't issues of - of having a - a second stream or something? That was - Was it - There was this business that, you know, we - we could use up the full forty-eight hundred bits, and -
mn007: Yeah. But I think they'r- I think we want to work on this. They also want to work on this, so. Uh. yeah. We - we will try MSG, but um, yeah. And they are t- I think they want to work on the second stream also, but more with some kind of multi-band or, well, what they call TRAP or generalized TRAP.
me013: Mm-hmm.
mn007: Um.
me013: OK. Do you remember when the next meeting is supposed to be? the next uh - In June. OK.
mn007: So. It's uh in June. Yeah.
me013: Yeah. Um. Yeah, the other thing is that you saw that - that mail about uh the VAD - VADs performing quite differently? That that uh So um. This - there was this experiment of uh "what if we just take the baseline?" set uh of features, just mel cepstra,
mn007: Mmm.
me013: and you inc- incorporate the different VADs. And it looks like the - the French VAD is actually uh better - significantly better.
me018: Improves the baseline?
me013: Yeah. Yeah.
mn007: Yeah but I don't know which VAD they use. Uh. If the use the small VAD I th- I think it's on - I think it's easy to do better because it doesn't work at all. So. I - I don't know which - which one. It's Pratibha that - that did this experiment.
fn002: Yeah.
mn007: Um. We should ask which VAD
fn002: I don't @@ . He - Actually, I think that he say with the good VAD of - from OGI
mn007: she used.
fn002: and with the Alcatel VAD. And the experiment was sometime better, sometime worse.
mn007: Yeah but I - it's uh - I think you were talking about the other mail that used VAD on the reference features.
me013: Yes.
mn007: Yeah.
fn002: I don't remember.
me013: And on that one, uh the French one is - was better. It was just better.
fn002: Mm-hmm.
me013: I mean it was enough better that - that it would uh account for a fair amount of the difference
mn007: Mm-hmm.
me013: between our performance, actually.
fn002: Mm-hmm.
me013: So. Uh. So if they have a better one, we should use it. I mean. You know? it's - you can't work on everything. Uh.
mn007: Yeah.
me013: Uh.
mn007: Yeah, so we should find out if it's really better. I mean if it -
me013: Yeah.
mn007: the - compared to the small or the big network.
fn002: Mm-hmm.
me013: Yeah.
mn007: And perhaps we can easily improve if - if we put like mean normalization before the - before the VAD. Because -
me013: Yeah.
mn007: as - as you've mentioned. Mmm.
me013: H- Hynek will be back in town uh the week after next, back - back in the country. So. And start - start organizing uh more visits and connections and so forth, and - uh
mn007: Mm-hmm.
me013: working towards June.
mn007: Yeah. Mm-hmm.
fn002: Also is Stephane was thinking that maybe it was useful to f- to think about uh voiced-unvoiced - to work uh here in voiced-unvoiced detection.
mn007: Yeah.
fn002: And we are looking in the uh signal.
mn007: Yeah. Yeah, my feeling is that um actually when we look at all the proposals, ev- everybody is still using some kind of spectral envelope and
me013: Right.
mn007: um
me013: No use of pitch uh basically. Yeah.
mn007: it's - Yeah, well, not pitch, but to look at the um fine - at the - at the high re- high resolution spectrum.
me013: Yeah.
mn007: So.
me013: Well, it -
mn007: We don't necessarily want to find the - the pitch of the - of the sound but uh - Cuz I have a feeling that when we look - when we look at the - just at the envelope there is no way you can tell if it's voiced and unvoiced, if there is some - It's - it's easy in clean speech because voiced sound are more low frequency and. So there would be more,
me013: Yeah.
mn007: uh - there is the first formant, which is the larger and then voiced sound are more high frequencies cuz it's frication and -
me013: Right.
mn007: But, yeah. When you have noise there is no um - if - if you have a low frequency noise it could be taken for - for voiced speech and.
me013: Yeah, you can make these mistakes, but - but -
mn007: So.
me018: Isn't there some other
mn007: S-
me018: uh d-
mn007: So I think that it - it would be good - Yeah, yeah, well, go - go on.
me018: Uh, I was just gonna say isn't there - aren't - aren't there lots of ideas for doing voice activity, or speech-nonspeech rather, um by looking at um, you know, uh I guess harmonics or looking across time -
me013: Well, I think he was talking about the voiced-unvoiced, though, right? So, not the speech-nonspeech.
mn007: Mmm.
me018: Yeah. Well even with e- uh w- ah
me013: Yeah.
me018: you know, uh even with the voiced-non-
mn007: Mmm.
me018: voiced-unvoiced um - I thought that you or somebody was talking about -
me013: Well. Uh yeah. B- We should let him finish what he w- he was gonna say, and -
mn007: So.
me018: OK. So go ahead.
mn007: Um yeah, so yeah, I think if we try to develop a second stream well, there would be one stream that is the envelope and the second, it could be interesting to have that's - something that's more related to the fine structure of the spectrum. And. Yeah, so I don't know. We were thinking about like using ideas from - from Larry Saul, have a good voice detector, have a good, well, voiced-speech detector, that's working on - on the FFT and uh Larry Saul could be an idea. We were are thinking about just
me013: U-
mn007: kind of uh taking the spectrum and computing the variance of - of the high resolution spectrum and
me013: So u- s- u-
mn007: things like this.
me013: OK. So - So many tell you something about that. Uh we had a guy here some years ago who did some work on um making use of voicing information uh to help in reducing the noise.
mn007: Yeah? Mm-hmm.
me013: So what he was doing is basically y- you - you do estimate the pitch. And um you - from that you - you estimate - or you estimate fine harmonic structure, whichev- ei- either way, it's more or less the same. But uh the thing is that um you then can get rid of things that are not - i- if there is strong harmonic structure, you can throw away stuff that's - that's non- harmonic.
mn007: Mm-hmm. Mm-hmm.
me013: And that - that is another way of getting rid of part of the noise
mn007: Yeah. Yeah.
me013: So um that's something that is sort of finer, brings in a little more information than just spectral subtraction. Um. And he had some - I mean, he did that sort of in combination with RASTA. It was kind of like RASTA was taking care of convolutional stuff and he was -
mn007: Mm-hmm. Mmm. Mm-hmm.
me013: and - and got some - some decent results doing that. So that - that's another - another way.
mn007: Yeah. Mmm.
me013: But yeah, there's - there's - Right. There's all these cues. We've
mn007: But -
me013: actually back when Chuck was here we did some voiced-unvoiced uh classification using a bunch of these, and - and uh works OK. Obviously it's not perfect but um - But the thing is that you can't -
mn007: Mm-hmm.
me013: given the constraints of this task, we can't, in a very nice way, feed forward to the recognizer the information - the probabilistic information that you might get about whether it's voiced or unvoiced, where w- we can't you know affect the - the uh distributions or anything.
mn007: Mm-hmm.
me013: But we - what we uh - I guess we could Yeah.
me018: Didn't the head dude send around that message? Yeah, I think you sent us all a copy of the message, where he was saying that - I- I'm not sure, exactly, what the gist of what he was saying, but something having to do with the voice activity detector and that it will - that people shouldn't put their own in or something. It was gonna be a -
me013: That - But - OK. So that's voice activity detector as opposed to voicing detector. So we're talking about something a little different.
mn007: They didn't. Mmm.
me018: Oh, I'm sorry. I - I missed that.
mn007: Mmm.
me013: Right? I guess what you could do, maybe this would be w- useful, if - if you have - if you view the second stream, yeah, before you - before you do KLT's and so forth, if you do view it as probabilities, and if it's an independent - So, if it's - if it's uh not so much envelope-based by fine-structure-based, uh looking at harmonicity or something like that, um if you get a probability from that information and then multiply it by - you know, multiply by all the voiced outputs and all the unvoiced outputs, you know,
mn007: Mm-hmm.
me013: then use that as the uh - take the log of that or uh pre- pre- uh - pre-nonlinearity,
mn007: Yeah. i- if -
me013: uh and do the KLT on the - on - on that, then that would - that would I guess
mn007: Yeah.
me013: be uh a reasonable use of independent information. So maybe that's what you meant. And then that would be -
mn007: Yeah, well, I was not thinking this - yeah, this could be an- yeah So you mean have some kind of probability for the v- the voicing and then
me013: R- Right. So you have a second neural net. It could be pretty small.
mn007: use a tandem system and
me013: Yeah. If you have a tandem system and then you have some kind of - it can be pretty small - net - we used - we d- did some of this stuff. Uh I - I did, some years ago, and the - and - and you use - the thing is to use information primarily that's different
mn007: Mm-hmm. Yeah.
me013: as you say, it's more fine-structure-based than - than envelope-based uh so then it you - you - you can pretty much guarantee it's stuff that you're not looking at very well with the other one, and uh then you only use for this one distinction.
mn007: Mm-hmm. Alright.
me013: And - and so now you've got a probability of the cases, and you've got uh the probability of the finer uh categories on the other side. You multiply them where appropriate and uh um
mn007: I see, yeah. Mm-hmm.
me013: if they really are from independent information sources then they should have different kinds of errors and roughly independent errors, and it's a good choice for -
mn007: Mm-hmm. Mm-hmm. Mm-hmm. Yeah.
me013: Uh. Yeah, that's a good idea.
mn007: Yeah. Because, yeah, well, spectral subtraction is good and we could u- we could use the fine structure to - to have a better estimate of the noise but still there is this issue with spectral subtraction that it seems to increase the variance of - of - of um
me013: Yeah.
mn007: Well it's this musical noise which is
me013: Right.
mn007: annoying if you d- you do some kind of on-line normalization after. So. Um. Yeah. Well. Spectral subtraction and on-line normalization don't seem to - to go together very well. I-
me013: Or if you do a spectral subtraction - do some spectral subtraction first and then do some on-line normalization then do some more spectral subtraction - I mean, maybe - maybe you can do it layers or something so it doesn't - doesn't hurt too much or something.
mn007: Ah, yeah.
me013: But it - but uh, anyway I think I was sort of arguing against myself there by giving that example uh I mean cuz I was already sort of
mn007: Yeah.
me013: suggesting that we should be careful about not spending too much time on exactly what they're doing In fact if you get - if you go into uh - a uh harmonics-related thing it's definitely going to be different than what they're doing and uh
mn007: Mm-hmm.
me013: uh should have some interesting properties in noise. Um. I know that when have people have done um sort of the obvious thing of taking uh your feature vector and adding in some variables which are pitch related or uh that - it hasn't - my impression it hasn't particularly helped.
mn007: It -
me013: Uh. Has not.
mn007: it i- has not, yeah.
me013: Yeah. But I think uh
mn007: Oh.
me013: that's - that's a question for this uh you know extending the feature vector versus having different streams.
mn007: Was it nois- noisy condition? the example that you -
me013: And - and it may not have been noisy conditions. Yeah. I - I don't remember the example but it was - it was on some DARPA data and some years ago and so it probably wasn't,
mn007: you just Yeah. Mm-hmm.
me013: actually
mn007: Mm-hmm. Yeah. But we were thinking, we discussed with Barry about this, and perhaps thinking - we were thinking about some kind of sheet- cheating experiment where
me013: Uh-huh.
mn007: we would use TIMIT and see if giving the d- uh, this voicing bit would help in - in terms of uh frame classification. Mmm.
me013: Why don't you - why don't you just do it with Aurora? Just any i- in - in each - in each frame
mn007: Yeah, but - but -
me006: We're -
mn007: B- but we cannot do the cheating, this cheating thing. Well.
me013: uh -
me006: We need labels.
me013: Why not?
mn007: Cuz we don't have - Well, for Italian perhaps we have, but we don't have this labeling for Aurora. We just have a labeling with word models but
me013: I see.
fn002: Not for foreigners .
mn007: not for phonemes.
me006: we don't have frame - frame level
fn002: Right.
me013: Um.
me006: transcriptions.
mn007: Um.
me013: But you could - I mean you can - you can align so that - It's not perfect, but if you - if you know what was said and -
me018: But the problem is that their models are all word level models. So there's no phone models that you get alignments for.
mn007: Yeah. Mm-hmm.
me013: Oh.
me018: You - So you could find out where the word boundaries are but that's about it.
me013: Yeah. I see.
me006: S- But we could use uh the - the noisy version that TIMIT, which
mn007: Yeah.
me006: you know, is similar to the - the noises found in the TI-digits
mn007: noise, yeah.
me006: um portion of Aurora.
mn007: Yeah, that's right, yep. Mmm. Well, I guess - I guess we can -
me013: Yeah.
mn007: we can say that it will help, but I don't know. If this voicing bit doesn't help, uh, I think we don't have to - to work more about this because -
me013: Uh.
mn007: Uh. It's just to know if it - how much i- it will help and to have an idea of
me013: Yeah. Right.
mn007: how much we can gain.
me013: I mean in experiments that we did a long time ago and different ta- it was probably Resource Management or something,
mn007: Mmm.
me013: um, I think you were getting something like still eight or nine percent error on the voicing, as I recall. And um,
me006: Another person's voice.
me013: so um what that said is that, sort of, left to its own devices, like without the - a strong language model and so forth, that you would - you would make significant number of errors just with your uh probabilistic machinery in deciding one oh
me018: It also - Yeah, the - though I think uh there was one problem with that in that, you know, we used canonical mapping so our truth may not have really been true to the acoustics.
me013: Uh-huh.
me006: Hmm.
me018: So.
mn007: Mmm.
me013: Yeah. Well back twenty years ago when I did this voiced-unvoiced stuff, we were getting more like ninety-seven or ninety- eight percent correct in voicing. But that was speaker- dependent actually.
mn007: Mm-hmm.
me013: We were doing training on a particular announcer and - and getting a
mn007: Mm-hmm.
me013: very good handle on the features. And we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and - and - and uh - and exhaustively searched all size subsets
me018: Wow!
me013: and - and uh - for - for that particular speaker and you'd find you know the five or six features which really did well on them.
mn007: Mm-hmm.
me013: And then doing - doing all of that we could get down to two or three percent error.
mn007: Mm-hmm.
me013: But that, again, was speaker- dependent with lots of feature selection and a very complex sort of thing. So I would - I would believe
mn007: Mmm.
me013: that uh it was quite likely that um looking at envelope only, that we'd be significantly worse than that.
mn007: Mm-hmm.
me013: Uh.
mn007: And the - all the - the SpeechCorders? what's the idea behind? Cuz they - they have to - Oh, they don't even have to detect voiced spe- speech?
me013: The modern ones don't do a - a simple switch. They work on the code book excitation. Yeah they do analysis-by-synthesis. They try - they - they try every - every possible excitation they have in their code book and find the one that matches best.
mn007: They just work on the code book and find out the best excitation. Yeah. Mmm. Alright. Yeah. So it would not help.
me013: Yeah.
me006: Hmm.
me013: Uh. O K.
me018: Can I just mention one other interesting thing?
me013: Yeah.
me018: Um. One of the ideas that we had come up with last week for things to try to improve the system - Um. Actually I - I s- we didn't - I guess I wrote this in after the meeting b- but the thought I had was um looking at the language model that's used in the HTK recognizer, which is basically just a big
me006: Mm-hmm.
me018: loop, right? So you - it goes "digit" and then that can be - either go to silence or go to another digit, which - That model would allow for the production of
fn002: Mm-hmm.
me018: infinitely long sequences of digits, right?
me013: Right.
me018: So. I thought "well I'm gonna just look at the - what actual digit strings do occur in the training data."
me013: Right.
me018: And the interesting thing was it turns out that there are no sequences of two-long or three-long digit strings in any of the Aurora training data. So it's either one, four, five, six, uh up to eleven, and then it skips and then there's some at sixteen.
me013: But what about the testing data?
me018: Um. I don't know. I didn't look at the test data yet. So.
me013: Yeah. I mean if there's some testing data that has - has - has two or three -
me018: Yeah. But I just thought that was a little odd, that there were no two or three long - Sorry. So I - I - just for the heck of it, I made a little grammar which um, you know, had it's separate path for each length digit string you could get. So there was a one-long path and there was a four-long and a five-long and I tried that and it got way worse. There were lots of deletions. So it was -
me013: Mm-hmm. Mm-hmm.
me018: you know, I - I didn't have any weights of these paths or - I didn't have anything like that. And I played with tweaking the word transition penalties a bunch, but I couldn't go anywhere.
me013: Mm-hmm.
me018: But um.
me013: Hmm.
me018: I thought "well if I only allow -" Yeah, I guess I should have looked at - to see how often there was a mistake where a two-long or a three-long path was actually put out as a hypothesis. Um. But.
me013: Hmm.
me018: So to do that right you'd probably want to have - allow for them all but then have weightings and things. So. I just thought that was a interesting thing about the data.
me013: OK. So we're gonna read some more digit strings I guess?
me018: Yeah. You want to go ahead, Morgan?
me013: Sure.