me013: Let's see. Test? Test? Yeah. OK.
mn007: Channel one.
me006: Hello? Hello?
fn002: Test.
me013: I was saying Hynek'll be here next week, uh, Wednesday through Friday - uh, through Saturday, and, um, I won't be here Thursday and Friday. But my suggestion is that, uh, at least for this meeting, people should go ahead, uh, cuz Hynek will be here, and, you know, we don't have any Czech accent yet, uh, as far as I know, so -
me018: OK.
me013: There we go. Um. So other than reading digits, what's our agenda?
me018: I don't really have, uh, anything new. Been working on Meeting Recorder stuff. So.
me013: OK. Um. Do you think that would be the case for next week also? Or is - is, uh - ? What's your projection on - ?
me018: Um.
me013: Cuz the one thing - the one thing that seems to me we really should try, if you hadn't tried it before, because it hadn't occurred to me - it was sort of an obvious thing - is, um, adjusting the, uh, sca- the scaling and, uh, insertion penalty sorta stuff.
me018: I did play with that, actually, a little bit. Um. What happens is, uh, when you get to the noisy stuff, you start getting lots of insertions.
me013: Right.
me018: And, um, so I've tried playing around a little bit with, um, the insertion penalties and things like that.
me013: Yeah.
me018: Um. I mean, it - it didn't make a whole lot of difference. Like for the well-matched case, it seemed like it was pretty good. Um. I could do more playing with that, though. And, uh - and see.
me013: But you were looking at mel cepstrum.
me018: Yes.
me013: Right.
me018: Oh, you're talking about for th- for our features.
me013: @@ Right. So, I mean, i- it- it's not the direction that you were working with that we were saying what's the - uh, what's the best you can do with - with mel cepstrum.
me018: Mmm.
me013: But, they raised a very valid point, which, I guess - So, to first order - I mean, you have other things you were gonna do, but to first order, I would say that the conclusion is that if you, um, do, uh, some monkeying around with, uh, the exact HTK training and @@ with, uh, you know, how many states and so forth, that it - it doesn't particularly improve the performance. In other words, that even though it sounds pretty dumb, just applying the same number of states to everything, more or less, no matter what language, isn't so bad. Right? And I guess you hadn't gotten to all the experiments you wanted to do with number of Gaussians,
me018: Right.
me013: but, um, let's just - If we had to - if we had to draw a conclusion on the information we have so far, we'd say something like that. Right?
me018: Mm-hmm.
me013: Uh, so the next question to ask, which is I think the one that - that - that Andreas was dre- addressing himself to in the lunch meeting, is, um, we're not supposed to adjust the back-end, but anybody using the system would.
me018: Yeah.
me013: So, if you were just adjusting the back-end, how much better would you do, uh, in noise? Uh, because the language scaling and insertion penalties and so forth are probably set to be about right for mel cepstrum.
me018: Mm-hmm.
me013: But, um, they're probably not at all set right for these things, particularly these things that look over, uh, larger time windows, in one way or another with - with LDA and KLT and neural nets and all these things. In the fa- past we've always found that we had to increase the insertion penalty to - to correspond to such things. So, I think that's, uh, @@ that's kind of a first-order thing that - that we should try.
me018: So for th- so the experiment is to, um, run our front-end like normal, with the default, uh, insertion penalties and so forth, and then tweak that a little bit and see how much of a difference it makes
me013: So by "our front-end" I mean take, you know, the Aurora-two s- take some version that Stephane has that is,
me018: if we were - Mm-hmm.
me013: you know, our current best version of something. Um. I mean, y- don't wanna do this over a hundred different things that they've tried but, you know, for some version that you say is a good one. You know? Um. How - how much, uh, does it improve if you actually adjust that?
me018: OK.
me013: But it is interesting. You say you - you have for the noisy - How about for the - for the mismatched or - or - or - or the - or the medium mismatched conditions? Have you - ? When you adjusted those numbers for mel cepstrum, did it - ?
me018: Uh, I - I don't remember off the top of my head. Um. Yeah. I didn't even write them down. I - I - I don't remember. I would need to - Well, I did write down, um - So, when I was doing - I just wrote down some numbers for the well-matched case.
me013: Yeah.
me018: Um. Looking at the - I wrote down what the deletions, substitutions, and insertions were, uh, for different numbers of states per phone.
me013: Yeah.
me018: Um, but, uh, that - that's all I wrote down.
me013: OK.
me018: So. I - I would - Yeah. I would need to do that.
me013: OK. So -
me018: I can do that for next week.
me013: Yeah. And, um - Yeah. Also, eh, eh, sometimes if you run behind on some of these things, maybe we can get someone else to do it and you can supervise or something. But -
me018: OK.
me013: but I think it would be - it'd be good to know that.
me018: I just need to get, um, front-end, uh, stuff from you or you point me to some files that you've already calculated.
mn007: Hmm. Yeah. Alright.
me013: OK. Uh.
me018: I probably will have time to do that and time to play a little bit with the silence
me013: Mm-hmm.
me018: model. So maybe I can have that for next week when Hynek's here.
me013: Yeah.
mn007: Mm-hmm.
me013: Yeah. Cuz, I mean, the - the other - That, in fact, might have been part of what, uh, the difference was - at least part of it that - that we were seeing. Remember we were seeing the SRI system was so much better than the tandem system.
me018: Hmm.
me013: Part of it might just be that the SRI system, they - they - they always adjust these things to be sort of optimized, and -
me018: Is there - ? I wonder if there's anything that we could do to the front-end that would affect the insertion -
me013: Yes. I think you can.
me018: What could you do?
me013: Well, um - uh, part of what's going on, um, is the, uh, the range of values. So, if you have something that has a much smaller range or a much larger range,
me018: Oh.
me013: and taking the appropriate root.
me018: Mm-hmm.
me013: You know? If something is kind of like the equivalent of a bunch of probabilities multiplied together, you can take a root of some sort. If it's like seven probabilities together, you can take the seventh root of it or something, or if it's in the log domain, divide it by seven.
me018: Mm-hmm.
me013: But - but, um, that has a similar effect because it changes the scale of the numbers - of the differences between different candidates from the acoustic model
me018: Oh, right. So that w- Right. So, in effect,
me013: as opposed to what's coming from the language model.
me018: that's changing the value of your insertion penalty.
me013: Yeah. I mean, it's more directly like the - the language scaling or the, uh - the model scaling or acoustic scaling,
me018: That's interesting.
me013: but you know that those things have kind of a similar effect to the insertion penalty anyway. They're a slightly different way of -
me018: Mm-hmm. Right.
me013: of handling it. So, um -
me018: So if we know what the insertion penalty is, then we can get an idea about what range our number should be in, so that they match with that.
me013: I think so. Yeah. Yeah. So that's why I think that's another reason other than curiosity as to why i- it would in fact be kinda neat to find out if we're way off.
me018: Mm-hmm.
me013: I mean, the other thing is, are- aren't we seeing - ? Y- y- I'm sure you've already looked at this bu- in these noisy cases, are - ? We are seeing lots of insertions. Right? The insertion number is quite high? @@ I know the VAD takes pre- care of part of that, but -
mn007: Yeah.
me018: Yeah. I've seen that with the mel cepstrum. I don't - I don't know about the Aurora front-end, but -
mn007: Yeah. I think it's much more balanced with, uh - when the front-end is more robust. Yeah. I could look at it - at this. Yeah.
me013: Yeah. Wha- what's a typical number?
mn007: Mm-hmm. I don't - I don't know.
me013: Do we - ? Oh, you - oh, you don't know. OK.
mn007: I don't have this in -
me013: I'm sure it's more balanced, but it - it - it wouldn't surprise me if there's still -
mn007: Mm-hmm.
me013: I mean, in - in the - the - the old systems we used to do, I - I -
mn007: Mm-hmm.
me013: uh, I remember numbers kind of like insertions being half the number of deletions, as being - and both numbers being - tend to be on the small side comparing to - to, uh, substitutions.
mn007: Mm-hmm.
me018: Well, this - the whole problem with insertions was what I think, um, we talked about when the guy from OGI came down that one time and - and that was when people were saying, well we should have a, uh, uh, voice activity detector -
me013: Right.
me018: that, because all that stuff that we're getting thr- the silence that's getting through is causing insertions. So.
mn007: Mmm.
me013: Right.
me018: I'll bet you there's still a lot of insertions.
mn007: Mm-hmm.
me013: Yeah. And it may be less of a critical thing. I mean, the fact that some get by may be less of a critical thing if you, uh, get things in the right range.
me018: Mm-hmm.
me013: So, I mean, the insertions is - is a symptom. It's a symptom that there's something, uh, wrong with the range.
me018: Right.
me013: But there's - uh, your - your - your substitutions tend to go up as well. So, uh, I - I - I think that,
me018: Mm-hmm.
me013: uh, the most obvious thing is just the insertions, @@ . But - Uh - um. If you're operating in the wrong range - I mean, that's why just in general, if you change what these - these penalties and scaling factors are, you reach some point that's a - that's a minimum. So. Um. Um. We do have to do well over a range of different conditions, some of which are noisier than others. Um. But, um, I think we may get a better handle on that if we - if we see - Um, I mean we ca- it's if we actually could pick a - a - a more stable value for the range of these features, it, um, uh, could - Uh - Even though it's - it's - it's true that in a real situation you can in fact adjust the - these - these scaling factors in the back-end, and it's ar- artificial here that we're not adjusting those, you certainly don't wanna be adjusting those all the time. And if you have a nice front-end that's in roughly the right range -
me018: Hmm.
me013: I remember after we got our stuff more or less together in the previous systems we built, that we tended to set those scaling factors at kind of a standard level, and we would rarely adjust them again, even though you could get a -
me018: Mm-hmm.
me013: for an evaluation you can get an extra point or something if you tweaked it a little bit. But, once we knew what rou- roughly the right operating range was, it was pretty stable, and - Uh, we might just not even be in the right operating range.
me018: So, would the - ? Uh, would a good idea be to try to map it into the same range that you get in the well-matched case? So, if we computed what the range was in well-matched, and then when we get our noisy conditions out we try to make it have the same range as - ?
me013: No. You don't wanna change it for different conditions. No. No. I - I - I - What - what I'm saying -
me018: Oh, I wasn't suggesting change it for different conditions. I was just saying that when we pick a range, we - we wanna pick a range that we map our numbers into - we should probably pick it based on
me013: Yeah.
me018: the range that we get in the well-matched case. Otherwise, I mean, what range are we gonna choose to - to map everything into?
me013: Well. It depends how much we wanna do gamesmanship and how much we wanna do - I mean, i- if he- it - to me, actually, even if you wanna be - play on the gamesmanship side, it can be kinda tricky. So, I mean, what you would do is set the - set the scaling factors, uh, so that you got the best number for this point four five times the - you know, and so on.
me018: Mm-hmm.
me013: But they might change that - those weightings.
me018: Yeah.
me013: Um. So - Uh - I just sorta think we need to explore the space.
me018: Mm-hmm.
me013: Just take a look at it a little bit. And we - we - we may just find that - that we're way off.
me018: OK. Mm-hmm.
me013: Maybe we're not. You know? As for these other things, it may turn out that, uh, it's kind of reasonable. But then - I mean, Andreas gave a very reasonable response, and he's probably not gonna be the only one who's gonna say this in the future - of, you know, people - people within this tight-knit community who are doing this evaluation are accepting, uh, more or less, that these are the rules.
me018: Yeah.
me013: But, people outside of it who look in at the broader picture are certainly gonna say "Well, wait a minute. You're doing all this standing on your head, uh, on the front-end, when all you could do is just adjust this in the back-end with one s- one knob." And
me018: Mm-hmm.
me013: so we have to at least, I think, determine that that's not true, which would be OK, or determine that it is true, in which case we want to adjust that and then continue with - with what we're doing. And as you say - as you point out -
me018: Right.
me013: finding ways to then compensate for that in the front-end also then becomes a priority for this particular test, and saying you don't have to do that.
me018: Mm-hmm.
me013: So. OK. So, uh - What's new with you?
mn007: Uh. So there's nothing new.
me013: Uh, what's old with you that's developed? You -
mn007: Um. I'm sorry?
me013: OK. What's old with you that has developed over the last week or two?
mn007: Mmm. Well, so we've been mainly working on the report and - and - Yeah.
me018: Mainly working on what?
mn007: On the report of the work that was already
me018: Oh.
mn007: done. Um. Mm-hmm. That's all.
me018: How about that - ? Any- anything new on the thing that, uh, you were working on with the, uh - ?
fn002: I don't have results yet.
me018: No results? Yeah.
me013: What was that?
me018: The - the, uh,
me006: Voicing thing.
me018: voicing detector.
me013: I mean, what- what's - what's going on now? What are you doing?
fn002: Uh, to try to found, nnn, robust feature for detect between voice and unvoice. And we - w- we try to use the variance of the es- difference between the FFT spectrum and mel filter bank spectrum.
me013: Yeah.
fn002: Uh, also the - another parameter is - relates with the auto-correlation function.
me013: Uh-huh.
fn002: Rze- energy and the variance a- also of the auto-correlation function.
me013: Uh-huh. So, that's - Yeah. That's what you were describing, I guess, a week or two ago. So.
fn002: Yeah. But we don't have res- we don't have result of the AURO- for Aurora yet. We need to train the neural network and -
me013: Mm-hmm. So you're training neural networks now?
fn002: No, not yet.
me013: So, what - wha- wh- wha- what- what's going on?
fn002: Well, we work in the report, too, because we have a lot of result, they are very dispersed, and was necessary to - to look in all the directory to - to -
me013: Uh-huh.
fn002: to give some more structure.
me013: So. B- So - Yeah. I- if I can summarize, basically what's going on is that you're going over a lot of material that you have generated in
mn007: Yea-
me013: furious fashion, f- generating many results and doing many experiments and trying to pull it together into some coherent
fn002: Hm-hmm.
me013: form to be able to see wha- see what happens. Yes?
mn007: Uh, y- yeah. Basically we- we've stopped, uh, experimenting, I mean. We're just writing some kind of technical report. And -
me018: Is this a report that's for Aurora? Or is it just like a tech report for ICSI, or - ?
fn002: No.
mn007: Yeah.
fn002: For ICSI.
me018: Ah. I see.
mn007: Yeah.
fn002: Just summary of the experiment and the conclusion and something like that.
me013: Yeah.
mn007: Mm-hmm.
me013: OK. So, my suggestion, though, is that you - you not necessarily finish that. But that you put it all together so that it's - you've got - you've got a clearer structure to it. You know what things are, you have things documented, you've looked things up that you needed to look up. So that, you know - so that such a thing can be written.
mn007: Mm-hmm.
me013: And, um - When - when - when do you leave again?
fn002: Uh, in July. First of July.
me013: First of July? OK. And that you figure on actually finishing it in - in June. Because, you know, you're gonna have another bunch of results to fit in there anyway.
mn007: Mm-hmm.
fn002: Mm-hmm.
me013: And right now it's kind of important that we actually
fn002: It's not.
me013: go forward with experiments. So - so, I - I think it's good to pause, and to gather everything together and make sure it's in good shape, so that other people can get access to it and so that it can go into a report in June. But I think to - to really work on - on fine-tuning the report n- at this point is - is probably bad timing,
mn007: Mm-hmm.
me013: I - I think.
mn007: Yeah. Well, we didn't - we just planned to work on it one week on this report, not - no more, anyway. Um.
me013: But you ma- you may really wanna add other things later anyway because you -
mn007: Yeah. Mm-hmm. Mmm.
me013: There's more to go?
mn007: Yeah. Well, so I don't know. There are small things that we started to - to do. But -
me018: Are you discovering anything, uh, that makes you scratch your head as you write this report, like why did we do that, or why didn't we do this, or - ?
mn007: Uh. Yeah. Yeah. And - Actually, there were some tables that were also with partial results. We just noticed that, wh- while gathering the result that for some conditions we didn't have everything.
me018: Mmm.
mn007: But anyway. Um. Yeah, yeah. We have, yeah, extracted actually the noises from the SpeechDat-Car. And so, we can train neural network with speech and these noises. Um. It's difficult to say what it will give, because when we look at the Aurora - the TI-digits experiments, um, they have these three conditions that have different noises, and apparently this system perform as well on the seen noises - on the unseen noises and on the seen noises. But, I think this is something we have to try anyway. So - adding the noises from - from the SpeechDat-Car. Um.
me013: That's -
mn007: @@
me013: that's, uh - that's permitted?
mn007: Uh. Well, OGI does - did that. Um. At some point they did that for - for the voice
fn002: Uh, for a v- VAD.
mn007: activity detector. Right? Um.
me018: Could you say it again? What - what exactly did they do?
mn007: They used some parts of the, um, Italian database to train the voice activity detector, I think.
me013: Yeah. I guess the thing is - Yeah. I guess that's a matter of interpretation. The rules as I understand it, is that in principle
mn007: It -
me013: the Italian and the Spanish and the English - no, Italian and the Finnish and the English ? - were development data
mn007: Yeah. And Spanish, yeah.
me013: on which you could adjust things. And the - and the German and Danish were the evaluation data.
mn007: Mm-hmm.
me013: And then when they finally actually evaluated things they used everything.
mn007: Yeah. That's right. Uh -
me013: So - Uh, and it is true that the performance, uh, on the German was - I mean, even though the improvement wasn't so good, the pre- the raw performance was really pretty good.
mn007: Mm-hmm.
me013: So - And, uh, it - it doesn't appear that there's strong evidence that even though things were somewhat tuned on those three or four languages, that - that going to a different language really hurt you. And the noises were not exactly the same. Right? Because it was taken from a different, uh - I mean they were different
mn007: Different cars. Yeah.
me013: drives. I mean, it was - it was actual different cars and so on. So.
mn007: Yeah.
me013: Um, it's somewhat tuned. It's tuned more than, you know, a - a - a - a -
mn007: Mm-hmm.
me013: You'd really like to have something that needed no particular noise at all, maybe just some white noise or something like that a- at most.
mn007: Mm-hmm.
me013: But that's not really what this contest is. So. Um, I guess it's OK.
mn007: Mm-hmm.
me018: I think it's -
me013: That's something I'd like to understand before we actually use something from it, because it would -
me018: it's probably something that, mmm, the - you know, the, uh, experiment designers didn't really think about, because I think most people aren't doing trained systems, or, you know, uh, systems that are like ours, where you actually use the data to build models. I mean, they just doing signal-processing. So.
mn007: Yeah.
me013: Well, it's true, except that, uh, that's what we used in Aurora one, and then they designed the things for Aurora-two knowing that we were doing that.
me018: Yeah. That's true.
me013: Um.
me018: And they didn't forbid us - right? - to build models on the data?
me013: No. But, I think - I think that it - it - it probably would be the case that if, say, we trained on Italian, uh, data and then, uh, we tested on Danish data and it did terribly, uh, that - that it would look bad. And I think someone would notice and would say "Well, look. This is not generalizing." I would hope tha- I would hope they would.
me018: Mm-hmm.
me013: Um. But, uh, it's true. You know, maybe there's parameters that other people have used - you know, th- that they have tuned in some way for other things. So it's - it's, uh - We should - we should - Maybe - that's maybe a topic - Especially if you talk with him when I'm not here, that's a topic you should discuss with Hynek to, you know, double check it's OK.
mn007: Mm-hmm.
me018: Do we know anything about the speakers for each of the, uh, training utterances?
mn007: What do you mean? We - we -
me018: Do you have speaker information?
me013: Social security number
me018: That would be good.
mn007: Like, we have male, female,
me018: Bank PIN.
fn002: Hmm.
me018: Just male f- female?
mn007: at least. Mmm.
me013: What kind of information do you mean?
me018: Well, I was thinking about things like, you know, gender, uh - you know, gender-specific nets and, uh, vocal tract length normalization.
mn007: Mm-hmm.
me018: Things like that. I d- I don't - I didn't know what information we have about the speakers that we could try to take advantage of.
mn007: Mm-hmm.
me013: Hmm. Uh. Right. I mean, again, i- if you had the whole system you were optimizing, that would be easy to see. But if you're supposedly just using a fixed back-end and you're just coming up with a feature vector, w- w- I'm not sure - I mean, having the two nets - Suppose you detected that it was male, it was female - you come up with different -
me018: Well, you could put them both in as separate streams or something. Uh.
mn007: Mm-hmm.
me013: Maybe.
me018: I don't know. I was just wondering if there was other information we could exploit.
mn007: Mm-hmm.
me013: Hmm. Yeah, it's an interesting thought. Maybe having something along the - I mean, you can't really do vocal tract normalization. But something that had some of that effect
me018: Yeah.
me013: being applied to the data in some way.
me018: Mm-hmm.
me013: Um.
mn007: Do you have something simple in mind for - I mean, vocal tract length normalization?
me018: Uh no. I hadn't - I hadn't thought - it was - thought too much about it, really. It just - something that popped into my head just now. And so I - I - I mean, you could maybe use the ideas - a similar idea to what they do in vocal tract length normalization. You know, you have some sort of a, uh, general speech model, you know, maybe just a mixture of Gaussians that you evaluate every utterance against, and then you see where each, you know, utterance - like, the likelihood of each utterance. You divide the - the range of the likelihoods up into discrete bins and then each bin's got some knob -
me013: Yeah. But just listen to yourself. I mean, that uh really doesn't sound like a real-time thing with less than two hundred milliseconds, uh,
me018: uh, setting. Yeah. Yeah.
mn007: Mm-hmm.
me013: latency that - and where you're not adjusting the statistical engine at all.
me018: Yeah. That's true.
me013: You know, that just -
me018: Right.
mn007: Hmm.
me018: Could be expensive.
me013: I mean - Yeah. No. Well not just expensive. I - I - I don't see how you could possibly do it. You can't look at the whole utterance and do anything. You know, you can only - Right?
me018: Oh, right.
me013: Each frame comes in and it's gotta go out the other end. So, uh -
me018: Right. So whatever it was, it would have to be uh sort of on a per frame basis.
me013: Yeah.
mn007: Mm-hmm.
me013: Yeah. I mean, you can do, um -
me018: Yeah.
me013: Fairly quickly you can do male female - f- male female stuff.
me018: Yeah.
me013: But as far as, I mean - Like I thought BBN did a thing with, uh, uh, vocal tract normalization a ways back. Maybe other people did too. With - with, uh, uh, l- trying to identify third formant - average third formant - using that as an indicator of -
me018: I don't know.
me013: So. You know, third formant - I- if you imagine that to first order what happens with, uh, changing vocal tract is that, uh, the formants get moved out by some proportion -
me018: Mm-hmm.
me013: So, if you had a first formant that was one hundred hertz before, if the fifty - if the vocal tract is fifty percent shorter, then it would be out at seven fifty hertz, and so on. So, that's a move of two hundred fifty hertz. Whereas the third formant which might have started off at twenty-five hundred hertz, you know, might be out to thirty-seven fifty, you know so it's at - So, although, you frequently get less distinct higher formants, it's still - third formant's kind of a reasonable compromise, and -
me018: Mm-hmm.
me013: So, I think, eh, if I recall correctly, they did something like that. And - and - But -
me018: Hmm.
me013: Um, that doesn't work for just having one frame or something. You know? That's more like looking at third formant over - over a turn or something like that, and -
me018: Yeah.
mn007: Mm-hmm. Mm-hmm.
me018: Right.
me013: Um. So. But on the other hand, male female is a - is a - is a much simpler categorization than figuring out a - a factor to,
me018: Mm-hmm.
me013: uh, squish or expand the - the spectrum. So, um. Y- you could imagine that - I mean, just like we're saying voiced-unvoiced is good to know - uh, male female is good to know also. Um.
me018: Mm-hmm.
me013: But, you'd have to figure out a way to - to - to, uh, incorporate it on the fly. Uh, I mean, I guess, as you say, one thing you could do is simply, uh, have the - the male and female output vectors - you know, tr- nets trained only on males and n- trained only on females or - or, uh, you know. But - Um. I don't know if that would really help, because you already have males and females and it's mm-hmm putting into one net. So is it - ?
me018: Is it balanced, um, in terms of gender - the data?
me013: Do you know?
mn007: Mmm. Almost, yeah.
me018: Hmm.
mn007: Mm-hmm.
me013: Hmm. OK. Y- you're - you were saying before - ?
mn007: Uh. Yeah. So, this noise, um - Yeah. The MSG - Um. Mmm. There is something - perhaps, I could spend some days to look at this thing, cuz it seems that when we train networks on - let's say, on TIMIT with MSG features, they - they look as good as networks trained on PLP. But, um, when they are used on - on the SpeechDat-Car data, it's not the case - oh, well . The MSG features are much worse, and so maybe they're, um, less - more sensitive to different recording conditions,
me013: Shouldn't be. They should be less so. R- right?
mn007: or - Shou- Yeah. But -
me013: Wh- ? But let me ask you this. What - what's the, um - ?
mn007: Mmm.
me013: Do you kno- recall if the insertions were - were higher with MSG?
mn007: I don't know. I cannot tell. But - It's - it - the - the error rate is higher.
me013: Yeah. But you should always look at insertions, deletions, and substitutions. So -
mn007: So, I don- Yeah. Mm-hmm. Mm-hmm.
me013: so, uh - @@ MSG is very, very dif- Eh, PLP is very much like mel cepstrum. MSG is very different from both of them.
mn007: Mm-hmm.
me013: So, if it's very different, then this is the sort of thing - I mean I'm really glad Andreas brought this point up. I sort of had forgotten to discuss it. Um. You always have to look at how this - uh, these adjustments, uh, affect things. And even though we're not allowed to do that, again we maybe could reflect that back to our use of the features. So if it - if in fact, uh - The problem might be that the range of the MSG features is quite different than the range
mn007: Mm-hmm. Mm-hmm.
me013: of the PLP or mel cepstrum. And you might wanna change that.
mn007: Mm-hmm. But - Yeah. But, it's d- it's after - Well, it's tandem features, so - Mmm.
me013: Yeah.
mn007: Yeah. We - we have estimation of post- posteriors
me013: Yeah.
mn007: with PLP and with MSG as input, so I don- Well. I don't know.
me013: That means they're between zero and one.
mn007: Mm-hmm.
me013: But i- it - it - it - it doesn't necessarily - You know, they could be, um - Do- doesn't tell you what the variance of the things is.
mn007: Mmm. Mm-hmm.
me013: Right? Cuz if you're taking the log of these things, it could be, uh - Knowing what the sum of the probabilities are, doesn't tell you what the sum of the logs are.
mn007: Mm-hmm. Yeah.
me013: So.
mn007: Yeah. So we should look at the likelihood, or - or what? Or - well, at the log, perhaps, and -
me013: Yeah. Yeah.
mn007: Mm-hmm.
me013: Or what - you know, what you're uh - the thing you're actually looking at.
mn007: Mm-hmm.
me013: So your - your - the values that are - are actually being fed into HTK.
mn007: Mm-hmm. But -
me013: What do they look like?
me018: No- And so th- the, uh - for the tandem system, the values that come out of the net don't go through the sigmoid. Right? They're sort of the pre-nonlinearity values?
me013: Right.
mn007: Yes.
me013: So they're kinda like log probabilities is what I was saying.
me018: And those - OK. And tho- that's what goes into HTK?
me013: Uh, almost. But then you actually do a KLT on them.
me018: OK.
me013: Um. They aren't normalized after that, are they?
mn007: Mmm. No, they are not - no.
me013: No. OK. So, um. Right. So the question is - Yeah. Whatever they are at that point, um, are they something for which taking a square root or cube root or fourth root or something like that is - is gonna be a good or a bad thing? So.
mn007: Mm-hmm.
me013: Uh, and that's something that nothing - nothing else after that is gonna - Uh, things are gonna scale it - Uh, you know, subtract things from it, scale it from it, but nothing will have that same effect. Um. So. Um.
me018: Yeah. Cuz if - if the log probs that are coming out
me013: Anyway, eh - Well, the -
me018: of the MSG are really big, the standard insertion penalty is gonna have very little effect
me013: Right.
me018: compared to, you know,
me013: Yeah.
me018: a smaller set of log probs.
me013: No. Again you don't really look at that. It's something - that, and then it's going through this transformation that's probably pretty close to - It's, eh, whatever the KLT is doing. But it's probably pretty close to what a - a - a discrete cosine transformation is doing.
me018: Yeah.
me013: But still it's - it's not gonna probably radically change the scale of things. I would think. And, uh - Yeah. It may be entirely off and - and it may be - at the very least it may be quite different for MSG than it is for mel cepstrum or PLP. So that would be - So the first thing I'd look at without adjusting anything would just be to go back to the experiment and look at the, uh, substitutions, insertions, and deletions. And if the - if the, uh - i- if there's a fairly large effect of the difference, say, uh, uh, the r- ratio between insertions and deletions
mn007: Mm-hmm.
me013: for the two cases then that would be, uh, an indicator that it might - might be in that direction.
mn007: Mm-hmm.
me013: Anything else?
mn007: Yeah. But, my - my point was more that it - it works sometimes and -
me013: Yeah.
mn007: but sometimes it doesn't work. So.
me013: Well.
mn007: And it works on TI-digits and on SpeechDat-Car it doesn't work, and -
me013: Yeah.
mn007: Mm-hmm. Yeah. Well.
me013: But, you know, some problems are harder than others, and -
mn007: Mm-hmm. Yeah.
me013: And, uh, sometimes, you know, there's enough evidence for something to work and then it's harder, it breaks. You know, so it's -
mn007: Mm-hmm.
me013: But it - but, um, i- it - it could be that when you say it works maybe we could be doing much better, even in TI-digits. Right?
mn007: Yeah. Yeah, sure.
me013: So.
mn007: Uh.
me013: Hmm? Yeah.
mn007: Yeah. Well, there is also the spectral subtraction, which, um - I think maybe we should, uh, try to integrate it in - in our system.
me013: Yeah.
mn007: Mmm. Mm-hmm.
me013: Right. O-
mn007: But, I think that would involve to - to mmm use a big - a - al- already a big bunch of the system of Ericsson. Because he has spectral subtraction, then it's followed by, um, other kind of processing that's - are dependent on the - uh, if it's speech or noi- or silence.
me013: Mm-hmm.
mn007: And there is this kind of spectral flattening after - if it's silence, and - and s- I - I think it's important, um, to reduce this musical noise and this - this increase of variance during silence portions. So. Well. This was in- this would involve to take almost everything from - from the - this proposal and - and then just add some kind of on-line normalization in - in the neural network.
me013: OK. Well, this'll be, I think, something for discussion with Hynek next week.
mn007: Mmm. Yeah. Mm-hmm.
me013: Yeah. OK. Right. So. How are, uh, uh - how are things going with what you're doing?
me026: Oh. Well, um, I took a lot of time just getting my taxes out of the way - multi-national taxes. So, I'm - I'm starting to write code now for my work but I don't have any results yet. Um, i- it would be good for me to talk to Hynek, I think, when he's here.
me013: Yeah.
me026: Do you know what his schedule will be like?
me013: Uh, he'll be around for three days.
me026: OK. So, y-
me013: Uh, we'll have a lot of time. So, uh -
me026: OK.
me013: Um. I'll, uh - You know, he's - he'll - he'll be talking with everybody in this room So.
me018: But you said you won't - you won't be here next Thursday?
me013: Not Thursday and Friday. Yeah. Cuz I will be at faculty retreat.
me018: Hmm.
me013: So. I'll try to connect with him and people as - as I can on - on Wednesday. But - Um. Oh, how'd taxes go? Taxes go OK?
me026: Mmm. Yeah.
me013: Yeah. Oh, good. Yeah. Yeah. That's just - that's - that's one of the big advantages of not making much money is the taxes are easier. Yeah.
me018: Unless you're getting money in two countries. They both want their cut.
me013: I think you are. Aren't you?
mn007: Hmm.
me026: Hmm. Yeah.
me018: Right?
me013: Yeah. Yeah. Huh. Canada w- Canada wants a cut?
me026: Mm-hmm.
me013: Have to do - So you - you have to do two returns?
me026: Mmm. W- uh, for two thousand I did. Yeah.
me013: Oh, oh. Yeah. For tw- That's right, ju-
me018: But not for this next year?
me013: Two thousand. Yeah. Probably not this next year, I guess. Yeah.
me026: Ye-
me013: Yeah.
me026: Um. Uh, I'll - I'll still have a bit of Canadian income but it'll be less complicated because I will not be a - considered a resident of Canada anymore, so I won't have to declare my American income on my Canadian return.
me013: OK. Alright. Uh. Barry, do you wanna say something about your stuff here?
me006: Oh, um. Right. I just, um, continuing looking at, uh, ph- uh, phonetic events, and, uh, this Tuesday gonna be, uh, meeting with John Ohala with Chuck to talk some more about these, uh, ph- um, phonetic events. Um, came up with, uh, a plan of attack, uh, gonna execute, and um - Yeah. It's - that's pretty much it.
me013: Oh, well. No- Um, why don't you say something about what it is?
me006: Oh, you - oh, you want - you want details. Hmm. OK.
me013: Well, we're all gathered here together. I thought we'd, you know -
me006: I was hoping I could wave my hands. Um. So, um. So, once wa- I - I was thinking getting - getting us a set of acoustic events to - um, to be able to distinguish between, uh, phones and words and stuff. And um, once we - we would figure out a set of these events that can be, you know, um, hand-labeled or - or derived, uh, from h- the hand-labeled phone targets. Um, we could take these events and, um, do some cheating experiments, um, where we feed, um, these events into an SRI system, um, eh, and evaluate its performance on a Switchboard task. Uh, yeah.
me026: Hey, Barry? Can you give an example of an event?
me006: Yeah. Sure. Um, I - I can give you an example of twenty-odd events. Um - So, he- In this paper, um, it's talking about phoneme recognition using acoustic events. So, things like frication or, uh, nasality.
me013: Whose paper is it?
me006: Um, this is a paper by Hubener and Cardson Benson - Bernds- Berndsen.
me013: Yeah. Huh. From, uh, University of Hamburg and Bielefeld.
me006: Mm-hmm.
me013: OK.
me006: Um.
me018: Yeah. I think the - just to expand a little bit on the idea of acoustic event. There's, um - in my mind, anyways, there's a difference between,
me006: Mm-hmm.
me018: um, acoustic features and acoustic events. And I think of acoustic features as being, um, things that linguists talk about, like, um -
me013: So, stuff that's not based on data.
me018: Stuff that's not based on data, necessarily. Right. That's not based on,
me013: Yeah. Oh, OK. Yeah. Yeah, OK.
me018: you know, acoustic data. So they talk about features for phones, like, uh, its height, its tenseness, laxness, things like that, which may or may not be all that easy to measure in the acoustic signal.
me006: Yeah. Mm-hmm.
me018: Versus an acoustic event, which is just some- something in the acoustic signal that is fairly easy to measure. Um. So it's, um - it's a little different, in - at least in my mind.
me013: I mean, when we did the SPAM work - I mean, there we had - we had this notion of an, uh, auditory - @@ auditory event.
me006: Good. That's great.
me013: And, uh, um, called them " avents ",
me018: Mm-hmm.
me013: uh, uh, uh, with an A at the front. Uh. And the - the - the idea was something that occurred that is important to a bunch of neurons somewhere. So.
me006: Mm-hmm.
me013: Um. A sudden change or a relatively rapid change in some spectral characteristic will - will do sort of this. I mean, there's certainly a bunch of - a bunch of places where you know that neurons are gonna fire because something novel has happened. That was - that was the main thing that we were focusing on there. But there's certainly other things beyond what we talked about there that aren't just sort of rapid changes, but -
me018: It's kinda like the difference between top-down and bottom-up.
me013: Yeah.
me018: I think of the acoustic - you know, phonetic features as being top-down. You know, you look at the phone and you say this phone is supposed to be - you know, have this feature, this feature, and this feature. Whether tha- those features show up in the acoustic signal is sort of irrelevant. Whereas, an acoustic event goes the other way. Here's the signal. Here's some event. What - ?
me006: Mm-hmm.
me018: And then that - you know, that may map to this phone sometimes, and sometimes it may not. It just depen- maybe depends on the context, things like that. And so it's sort of a different way of looking.
me013: Mm-hmm. Mm-hmm.
me006: Yeah. So. Yeah.
me026: OK.
me006: Mm-hmm. Um - Using these - these events, um, you know, we can - we can perform these - these, uh, cheating experiments. See how - how - how good they are, um, in, um - in terms of phoneme recognition or word recognition. And, um - and then from that point on, I would, uh, s- design robust event detectors, um, in a similar, um, wa- spirit that Saul has done w- uh, with his graphical models, and this - this probabilistic AND-OR model that he uses. Um, eh, try to extend it to, um - to account for other - other phenomena like, um, CMR co-modulation release. And, um - and maybe also investigate ways to - to modify the structure of these models, um, in a data-driven way, uh, similar to the way that, uh, Jeff - Jeff, uh, Bilmes did his work. Um, and while I'm - I'm doing these, um, event detectors, you know, I can ma- mea- measure my progress by comparing, um, the error rates in clean and noisy conditions to something like, uh, neural nets. Um, and - So - so, once we have these - these, uh, event detectors, um, we could put them together and - and feed the outputs of the event detectors into - into the SRI, um, HMM - HMM system, and, um - and test it on - on Switchboard or, um, maybe even Aurora stuff. And, that's pretty much the - the big picture of - of um, the plan.
me013: By the way, um, there's, uh, a couple people who are gonna be here - I forget if I already told you this, but, a couple people who are gonna be here for six months. Uh -
me006: Mm-hmm.
me013: uh, there's a Professor Kollmeier, uh, from Germany who's, uh, uh, quite big in the, uh, hearing-aid signal-processing area and, um, Michael Kleinschmidt, who's worked with him, who also looks at auditory properties inspired by various, uh, brain function things.
me006: Hmm.
me013: So, um, um, I think they'll be interesting to talk to, in this sort of issue as these detectors are - are, uh, developing.
me006: Hmm. OK.
me013: So, he looks at interesting - interesting things in - in the - different ways of looking at spectra in order to - to get various speech properties out. So.
me006: OK.
me013: OK. Well, short meeting, but that's OK. And, uh, we might as well do our digits. And like I say, I - I encourage you to go ahead and meet, uh, next week with, uh, uh, Hynek. Alright, I'll - I'll start. It's, uh, one thirty-five. seventeen OK-